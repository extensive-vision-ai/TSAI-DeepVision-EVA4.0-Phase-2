{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning-Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1usvLGGYQm-Y2fz7a9aYWbyBPpFjvDZx-",
      "authorship_tag": "ABX9TyOjbOvu0UJACRUL2/5h1jmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepVision-EVA4.0-Phase-2/blob/master/12-ImageCaptioning/ImageCaptioning_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb1BcVqrYFIi",
        "outputId": "b69ef6c1-1977-4f47-cc69-2b3ea635fca7"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 23 18:31:52 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHzSkw77ux1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ffcaf13-17d2-448e-d407-fec538fe2f3a"
      },
      "source": [
        "! git clone https://github.com/satyajitghana/a-PyTorch-Tutorial-to-Image-Captioning"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'a-PyTorch-Tutorial-to-Image-Captioning'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 307 (delta 11), reused 19 (delta 7), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (307/307), 12.91 MiB | 36.73 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igkl-v0vav9u"
      },
      "source": [
        "mv a-PyTorch-Tutorial-to-Image-Captioning img_caption"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrLogTatvqW8"
      },
      "source": [
        "import gdown"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5bfJjikwMcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "d15c9bb7-2d33-4ce9-cd08-a28e1bb9b4da"
      },
      "source": [
        "url = 'https://drive.google.com/uc?id=1zc_Qqe4SMjFAOH59mt8zY7qW8AIYCUYq'\n",
        "output = 'flickr8k.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zc_Qqe4SMjFAOH59mt8zY7qW8AIYCUYq\n",
            "To: /content/flickr8k.zip\n",
            "2.23GB [00:22, 97.3MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flickr8k.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "7Du3T3nNYoQX",
        "outputId": "8da1dab1-9e02-4169-b2bf-121eb11218b8"
      },
      "source": [
        "url = 'https://drive.google.com/uc?id=1QH6oUmeYeqNgu1Cbu3HVYRT35B-wEt3q'\n",
        "output = 'caption_dataset.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QH6oUmeYeqNgu1Cbu3HVYRT35B-wEt3q\n",
            "To: /content/caption_dataset.zip\n",
            "36.7MB [00:00, 121MB/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'caption_dataset.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0w36H-mYS3l"
      },
      "source": [
        "! unzip -q flickr8k.zip"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVeM4jYQYtuz"
      },
      "source": [
        "! unzip -q caption_dataset.zip"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j73makmKY3Xw"
      },
      "source": [
        "from img_caption.utils import create_input_files"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM4vOv33cMNd"
      },
      "source": [
        "! mkdir -p caption_data_flickr8k"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g5Jrk5_ZkLX",
        "outputId": "9b01ea8b-f80a-4b8f-d086-be626f538575"
      },
      "source": [
        "create_input_files(\n",
        "    dataset='flickr8k',\n",
        "    karpathy_json_path=\"dataset_flickr8k.json\",\n",
        "    image_folder=\"Flickr8k_Dataset/Flicker8k_Dataset\", \n",
        "    captions_per_image=5,\n",
        "    min_word_freq=5, \n",
        "    output_folder=\"caption_data_flickr8k\", \n",
        "    max_len=50\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 11/6000 [00:00<00:58, 103.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading TRAIN images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6000/6000 [00:54<00:00, 110.31it/s]\n",
            "  1%|          | 12/1000 [00:00<00:08, 110.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading VAL images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 111.10it/s]\n",
            "  1%|          | 12/1000 [00:00<00:08, 119.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Reading TEST images and captions, storing to file...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 110.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-haTBJOSeMgc"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPZ1KozoeL70"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14, backbone=\"resnet101\"):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        if backbone == \"resnet101\":\n",
        "            resnet = torchvision.models.resnet101(\n",
        "                pretrained=True\n",
        "            )  # pretrained ImageNet ResNet-101\n",
        "        elif backbone == \"resnet18\":\n",
        "            resnet = torchvision.models.resnet18(\n",
        "                pretrained=True\n",
        "            )  # pretrained ImageNet ResNet-18\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d(\n",
        "            (encoded_image_size, encoded_image_size)\n",
        "        )\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(\n",
        "            out\n",
        "        )  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(\n",
        "            0, 2, 3, 1\n",
        "        )  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(\n",
        "            encoder_dim, attention_dim\n",
        "        )  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(\n",
        "            decoder_dim, attention_dim\n",
        "        )  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(\n",
        "            attention_dim, 1\n",
        "        )  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(\n",
        "            2\n",
        "        )  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(\n",
        "            dim=1\n",
        "        )  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_dim,\n",
        "        embed_dim,\n",
        "        decoder_dim,\n",
        "        vocab_size,\n",
        "        encoder_dim=2048,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(\n",
        "            encoder_dim, decoder_dim, attention_dim\n",
        "        )  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(\n",
        "            embed_dim + encoder_dim, decoder_dim, bias=True\n",
        "        )  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(\n",
        "            decoder_dim, encoder_dim\n",
        "        )  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(\n",
        "            decoder_dim, vocab_size\n",
        "        )  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(\n",
        "            batch_size, -1, encoder_dim\n",
        "        )  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(\n",
        "            dim=0, descending=True\n",
        "        )\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(\n",
        "            encoded_captions\n",
        "        )  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(\n",
        "            device\n",
        "        )\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(\n",
        "                encoder_out[:batch_size_t], h[:batch_size_t]\n",
        "            )\n",
        "            gate = self.sigmoid(\n",
        "                self.f_beta(h[:batch_size_t])\n",
        "            )  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat(\n",
        "                    [embeddings[:batch_size_t, t, :], attention_weighted_encoding],\n",
        "                    dim=1,\n",
        "                ),\n",
        "                (h[:batch_size_t], c[:batch_size_t]),\n",
        "            )  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z-E91_hdrok"
      },
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "# from models import Encoder, DecoderWithAttention\n",
        "from img_caption.datasets import *\n",
        "from img_caption.utils import *\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def train(\n",
        "    train_loader,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    criterion,\n",
        "    encoder_optimizer,\n",
        "    decoder_optimizer,\n",
        "    epoch,\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to GPU, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(\n",
        "            imgs, caps, caplens\n",
        "        )\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        # pack_padded_sequence is an easy trick to do this\n",
        "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
        "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1.0 - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop.\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print(\n",
        "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
        "                \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
        "                \"Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
        "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
        "                \"Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\".format(\n",
        "                    epoch,\n",
        "                    i,\n",
        "                    len(train_loader),\n",
        "                    batch_time=batch_time,\n",
        "                    data_time=data_time,\n",
        "                    loss=losses,\n",
        "                    top5=top5accs,\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
        "    # solves the issue #57\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "\n",
        "            # Move to device, if available\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            if encoder is not None:\n",
        "                imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(\n",
        "                imgs, caps, caplens\n",
        "            )\n",
        "\n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
        "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1.0 - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print(\n",
        "                    \"Validation: [{0}/{1}]\\t\"\n",
        "                    \"Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
        "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
        "                    \"Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t\".format(\n",
        "                        i,\n",
        "                        len(val_loader),\n",
        "                        batch_time=batch_time,\n",
        "                        loss=losses,\n",
        "                        top5=top5accs,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(\n",
        "                        lambda c: [\n",
        "                            w\n",
        "                            for w in c\n",
        "                            if w not in {word_map[\"<start>\"], word_map[\"<pad>\"]}\n",
        "                        ],\n",
        "                        img_caps,\n",
        "                    )\n",
        "                )  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][: decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            \"\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n\".format(\n",
        "                loss=losses, top5=top5accs, bleu=bleu4\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return bleu4\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMnYEf90e1Iq",
        "outputId": "8091a6aa-8d80-4191-8a54-99ece9f5355f"
      },
      "source": [
        "# Data parameters\n",
        "data_folder = (\n",
        "    \"/content/caption_data_flickr8k\"  # folder with data files saved by create_input_files.py\n",
        ")\n",
        "data_name = \"flickr8k_5_cap_per_img_5_min_word_freq\"  # base name shared by data files\n",
        "\n",
        "# Model parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "backbone = \"resnet18\"\n",
        "encoder_dim = 512 # dimension from the resnet backbone\n",
        "dropout = 0.5\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0\n",
        "epochs = 20  # number of epochs to train for (if early stopping is not triggered)\n",
        "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "grad_clip = 5.0  # clip gradients at an absolute value of\n",
        "alpha_c = (\n",
        "    1.0  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
        ")\n",
        "best_bleu4 = 0.0  # BLEU-4 score right now\n",
        "print_freq = 1000  # print training/validation stats every __ batches\n",
        "fine_tune_encoder = True  # fine-tune encoder?\n",
        "checkpoint = None  # path to checkpoint, None if none\n",
        "\n",
        "# Read word map\n",
        "word_map_file = os.path.join(data_folder, \"WORDMAP_\" + data_name + \".json\")\n",
        "with open(word_map_file, \"r\") as j:\n",
        "    word_map = json.load(j)\n",
        "\n",
        "# Initialize / load checkpoint\n",
        "if checkpoint is None:\n",
        "    decoder = DecoderWithAttention(\n",
        "        attention_dim=attention_dim,\n",
        "        embed_dim=emb_dim,\n",
        "        decoder_dim=decoder_dim,\n",
        "        vocab_size=len(word_map),\n",
        "        dropout=dropout,\n",
        "        encoder_dim=encoder_dim\n",
        "    )\n",
        "    decoder_optimizer = torch.optim.Adam(\n",
        "        params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "        lr=decoder_lr,\n",
        "    )\n",
        "    encoder = Encoder(\n",
        "        backbone = backbone\n",
        "    )\n",
        "    encoder.fine_tune(fine_tune_encoder)\n",
        "    encoder_optimizer = (\n",
        "        torch.optim.Adam(\n",
        "            params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "            lr=encoder_lr,\n",
        "        )\n",
        "        if fine_tune_encoder\n",
        "        else None\n",
        "    )\n",
        "\n",
        "else:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    epochs_since_improvement = checkpoint[\"epochs_since_improvement\"]\n",
        "    best_bleu4 = checkpoint[\"bleu-4\"]\n",
        "    decoder = checkpoint[\"decoder\"]\n",
        "    decoder_optimizer = checkpoint[\"decoder_optimizer\"]\n",
        "    encoder = checkpoint[\"encoder\"]\n",
        "    encoder_optimizer = checkpoint[\"encoder_optimizer\"]\n",
        "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.Adam(\n",
        "            params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "            lr=encoder_lr,\n",
        "        )\n",
        "\n",
        "# Move to GPU, if available\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Custom dataloaders\n",
        "normalize = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset(\n",
        "        data_folder, data_name, \"TRAIN\", transform=transforms.Compose([normalize])\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset(\n",
        "        data_folder, data_name, \"VAL\", transform=transforms.Compose([normalize])\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "\n",
        "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "    if epochs_since_improvement == 20:\n",
        "        break\n",
        "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "        if fine_tune_encoder:\n",
        "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
        "\n",
        "    # One epoch's training\n",
        "    train(\n",
        "        train_loader=train_loader,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        criterion=criterion,\n",
        "        encoder_optimizer=encoder_optimizer,\n",
        "        decoder_optimizer=decoder_optimizer,\n",
        "        epoch=epoch,\n",
        "    )\n",
        "\n",
        "    # One epoch's validation\n",
        "    recent_bleu4 = validate(\n",
        "        val_loader=val_loader, encoder=encoder, decoder=decoder, criterion=criterion\n",
        "    )\n",
        "\n",
        "    # Check if there was an improvement\n",
        "    is_best = recent_bleu4 > best_bleu4\n",
        "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(\n",
        "        data_name,\n",
        "        epoch,\n",
        "        epochs_since_improvement,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        encoder_optimizer,\n",
        "        decoder_optimizer,\n",
        "        recent_bleu4,\n",
        "        is_best,\n",
        "    )"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/938]\tBatch Time 0.403 (0.403)\tData Load Time 0.166 (0.166)\tLoss 8.7939 (8.7939)\tTop-5 Accuracy 0.000 (0.000)\n",
            "Validation: [0/157]\tBatch Time 0.231 (0.231)\tLoss 4.2039 (4.2039)\tTop-5 Accuracy 58.172 (58.172)\t\n",
            "\n",
            " * LOSS - 4.190, TOP-5 ACCURACY - 61.576, BLEU-4 - 0.12065293881012207\n",
            "\n",
            "Epoch: [1][0/938]\tBatch Time 0.401 (0.401)\tData Load Time 0.181 (0.181)\tLoss 4.2463 (4.2463)\tTop-5 Accuracy 60.432 (60.432)\n",
            "Validation: [0/157]\tBatch Time 0.234 (0.234)\tLoss 3.9619 (3.9619)\tTop-5 Accuracy 64.286 (64.286)\t\n",
            "\n",
            " * LOSS - 3.942, TOP-5 ACCURACY - 64.900, BLEU-4 - 0.1327638462965481\n",
            "\n",
            "Epoch: [2][0/938]\tBatch Time 0.391 (0.391)\tData Load Time 0.180 (0.180)\tLoss 3.5515 (3.5515)\tTop-5 Accuracy 71.795 (71.795)\n",
            "Validation: [0/157]\tBatch Time 0.236 (0.236)\tLoss 3.9826 (3.9826)\tTop-5 Accuracy 61.139 (61.139)\t\n",
            "\n",
            " * LOSS - 3.834, TOP-5 ACCURACY - 66.194, BLEU-4 - 0.14118322832068458\n",
            "\n",
            "Epoch: [3][0/938]\tBatch Time 0.415 (0.415)\tData Load Time 0.176 (0.176)\tLoss 3.3658 (3.3658)\tTop-5 Accuracy 73.297 (73.297)\n",
            "Validation: [0/157]\tBatch Time 0.229 (0.229)\tLoss 3.8045 (3.8045)\tTop-5 Accuracy 66.000 (66.000)\t\n",
            "\n",
            " * LOSS - 3.775, TOP-5 ACCURACY - 67.197, BLEU-4 - 0.14451087036018873\n",
            "\n",
            "Epoch: [4][0/938]\tBatch Time 0.387 (0.387)\tData Load Time 0.178 (0.178)\tLoss 3.4978 (3.4978)\tTop-5 Accuracy 69.892 (69.892)\n",
            "Validation: [0/157]\tBatch Time 0.232 (0.232)\tLoss 3.8642 (3.8642)\tTop-5 Accuracy 64.489 (64.489)\t\n",
            "\n",
            " * LOSS - 3.755, TOP-5 ACCURACY - 67.534, BLEU-4 - 0.1444858594970811\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "Epoch: [5][0/938]\tBatch Time 0.418 (0.418)\tData Load Time 0.173 (0.173)\tLoss 3.1936 (3.1936)\tTop-5 Accuracy 75.000 (75.000)\n",
            "Validation: [0/157]\tBatch Time 0.221 (0.221)\tLoss 3.6715 (3.6715)\tTop-5 Accuracy 69.565 (69.565)\t\n",
            "\n",
            " * LOSS - 3.748, TOP-5 ACCURACY - 67.556, BLEU-4 - 0.14335803787790724\n",
            "\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n",
            "Epoch: [6][0/938]\tBatch Time 0.404 (0.404)\tData Load Time 0.182 (0.182)\tLoss 3.1302 (3.1302)\tTop-5 Accuracy 74.674 (74.674)\n",
            "Validation: [0/157]\tBatch Time 0.230 (0.230)\tLoss 3.7843 (3.7843)\tTop-5 Accuracy 68.719 (68.719)\t\n",
            "\n",
            " * LOSS - 3.745, TOP-5 ACCURACY - 67.641, BLEU-4 - 0.14262990858062782\n",
            "\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n",
            "Epoch: [7][0/938]\tBatch Time 0.418 (0.418)\tData Load Time 0.176 (0.176)\tLoss 2.9071 (2.9071)\tTop-5 Accuracy 80.916 (80.916)\n",
            "Validation: [0/157]\tBatch Time 0.239 (0.239)\tLoss 3.7800 (3.7800)\tTop-5 Accuracy 63.852 (63.852)\t\n",
            "\n",
            " * LOSS - 3.762, TOP-5 ACCURACY - 67.675, BLEU-4 - 0.14831231336895934\n",
            "\n",
            "Epoch: [8][0/938]\tBatch Time 0.408 (0.408)\tData Load Time 0.179 (0.179)\tLoss 2.6681 (2.6681)\tTop-5 Accuracy 84.848 (84.848)\n",
            "Validation: [0/157]\tBatch Time 0.259 (0.259)\tLoss 4.0453 (4.0453)\tTop-5 Accuracy 65.816 (65.816)\t\n",
            "\n",
            " * LOSS - 3.780, TOP-5 ACCURACY - 67.492, BLEU-4 - 0.14139636819762924\n",
            "\n",
            "\n",
            "Epochs since last improvement: 1\n",
            "\n",
            "Epoch: [9][0/938]\tBatch Time 0.412 (0.412)\tData Load Time 0.169 (0.169)\tLoss 2.5447 (2.5447)\tTop-5 Accuracy 85.791 (85.791)\n",
            "Validation: [0/157]\tBatch Time 0.229 (0.229)\tLoss 3.9354 (3.9354)\tTop-5 Accuracy 66.133 (66.133)\t\n",
            "\n",
            " * LOSS - 3.807, TOP-5 ACCURACY - 67.544, BLEU-4 - 0.14348413880451188\n",
            "\n",
            "\n",
            "Epochs since last improvement: 2\n",
            "\n",
            "Epoch: [10][0/938]\tBatch Time 0.417 (0.417)\tData Load Time 0.179 (0.179)\tLoss 2.5542 (2.5542)\tTop-5 Accuracy 83.862 (83.862)\n",
            "Validation: [0/157]\tBatch Time 0.238 (0.238)\tLoss 3.6314 (3.6314)\tTop-5 Accuracy 71.625 (71.625)\t\n",
            "\n",
            " * LOSS - 3.811, TOP-5 ACCURACY - 67.363, BLEU-4 - 0.14258880882540684\n",
            "\n",
            "\n",
            "Epochs since last improvement: 3\n",
            "\n",
            "Epoch: [11][0/938]\tBatch Time 0.393 (0.393)\tData Load Time 0.184 (0.184)\tLoss 2.5166 (2.5166)\tTop-5 Accuracy 84.326 (84.326)\n",
            "Validation: [0/157]\tBatch Time 0.233 (0.233)\tLoss 3.6678 (3.6678)\tTop-5 Accuracy 66.856 (66.856)\t\n",
            "\n",
            " * LOSS - 3.863, TOP-5 ACCURACY - 66.952, BLEU-4 - 0.14184713319199074\n",
            "\n",
            "\n",
            "Epochs since last improvement: 4\n",
            "\n",
            "Epoch: [12][0/938]\tBatch Time 0.401 (0.401)\tData Load Time 0.181 (0.181)\tLoss 2.3981 (2.3981)\tTop-5 Accuracy 90.083 (90.083)\n",
            "Validation: [0/157]\tBatch Time 0.234 (0.234)\tLoss 3.9646 (3.9646)\tTop-5 Accuracy 65.535 (65.535)\t\n",
            "\n",
            " * LOSS - 3.887, TOP-5 ACCURACY - 66.781, BLEU-4 - 0.13757902537497071\n",
            "\n",
            "\n",
            "Epochs since last improvement: 5\n",
            "\n",
            "Epoch: [13][0/938]\tBatch Time 0.427 (0.427)\tData Load Time 0.184 (0.184)\tLoss 2.4536 (2.4536)\tTop-5 Accuracy 87.903 (87.903)\n",
            "Validation: [0/157]\tBatch Time 0.235 (0.235)\tLoss 3.7468 (3.7468)\tTop-5 Accuracy 67.486 (67.486)\t\n",
            "\n",
            " * LOSS - 3.904, TOP-5 ACCURACY - 66.563, BLEU-4 - 0.1386582154962589\n",
            "\n",
            "\n",
            "Epochs since last improvement: 6\n",
            "\n",
            "Epoch: [14][0/938]\tBatch Time 0.421 (0.421)\tData Load Time 0.194 (0.194)\tLoss 2.3753 (2.3753)\tTop-5 Accuracy 86.650 (86.650)\n",
            "Validation: [0/157]\tBatch Time 0.254 (0.254)\tLoss 3.9205 (3.9205)\tTop-5 Accuracy 65.012 (65.012)\t\n",
            "\n",
            " * LOSS - 3.950, TOP-5 ACCURACY - 66.243, BLEU-4 - 0.1376592404095196\n",
            "\n",
            "\n",
            "Epochs since last improvement: 7\n",
            "\n",
            "Epoch: [15][0/938]\tBatch Time 0.415 (0.415)\tData Load Time 0.183 (0.183)\tLoss 2.2112 (2.2112)\tTop-5 Accuracy 90.954 (90.954)\n",
            "Validation: [0/157]\tBatch Time 0.236 (0.236)\tLoss 4.4174 (4.4174)\tTop-5 Accuracy 60.753 (60.753)\t\n",
            "\n",
            " * LOSS - 4.010, TOP-5 ACCURACY - 65.924, BLEU-4 - 0.13668649646786177\n",
            "\n",
            "\n",
            "Epochs since last improvement: 8\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000320\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.000080\n",
            "\n",
            "Epoch: [16][0/938]\tBatch Time 0.431 (0.431)\tData Load Time 0.188 (0.188)\tLoss 2.2151 (2.2151)\tTop-5 Accuracy 91.990 (91.990)\n",
            "Validation: [0/157]\tBatch Time 0.259 (0.259)\tLoss 4.0330 (4.0330)\tTop-5 Accuracy 67.442 (67.442)\t\n",
            "\n",
            " * LOSS - 4.027, TOP-5 ACCURACY - 65.830, BLEU-4 - 0.13580015540668117\n",
            "\n",
            "\n",
            "Epochs since last improvement: 9\n",
            "\n",
            "Epoch: [17][0/938]\tBatch Time 0.411 (0.411)\tData Load Time 0.179 (0.179)\tLoss 2.0494 (2.0494)\tTop-5 Accuracy 92.982 (92.982)\n",
            "Validation: [0/157]\tBatch Time 0.235 (0.235)\tLoss 4.1062 (4.1062)\tTop-5 Accuracy 66.578 (66.578)\t\n",
            "\n",
            " * LOSS - 4.064, TOP-5 ACCURACY - 65.554, BLEU-4 - 0.13279600302385497\n",
            "\n",
            "\n",
            "Epochs since last improvement: 10\n",
            "\n",
            "Epoch: [18][0/938]\tBatch Time 0.423 (0.423)\tData Load Time 0.179 (0.179)\tLoss 1.9900 (1.9900)\tTop-5 Accuracy 93.113 (93.113)\n",
            "Validation: [0/157]\tBatch Time 0.256 (0.256)\tLoss 3.6893 (3.6893)\tTop-5 Accuracy 71.391 (71.391)\t\n",
            "\n",
            " * LOSS - 4.124, TOP-5 ACCURACY - 65.428, BLEU-4 - 0.13102131816883883\n",
            "\n",
            "\n",
            "Epochs since last improvement: 11\n",
            "\n",
            "Epoch: [19][0/938]\tBatch Time 0.413 (0.413)\tData Load Time 0.187 (0.187)\tLoss 2.0983 (2.0983)\tTop-5 Accuracy 92.877 (92.877)\n",
            "Validation: [0/157]\tBatch Time 0.236 (0.236)\tLoss 3.9936 (3.9936)\tTop-5 Accuracy 65.952 (65.952)\t\n",
            "\n",
            " * LOSS - 4.171, TOP-5 ACCURACY - 65.138, BLEU-4 - 0.1332856777868179\n",
            "\n",
            "\n",
            "Epochs since last improvement: 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_BcuPJZfpsU",
        "outputId": "46c1d92e-2eb1-4cae-be68-97b6a9212b16"
      },
      "source": [
        "! zip -r caption_data_flickr8k.zip caption_data_flickr8k"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: caption_data_flickr8k/ (stored 0%)\n",
            "  adding: caption_data_flickr8k/TRAIN_IMAGES_flickr8k_5_cap_per_img_5_min_word_freq.hdf5 (deflated 17%)\n",
            "  adding: caption_data_flickr8k/TRAIN_CAPTIONS_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 91%)\n",
            "  adding: caption_data_flickr8k/VAL_IMAGES_flickr8k_5_cap_per_img_5_min_word_freq.hdf5 (deflated 17%)\n",
            "  adding: caption_data_flickr8k/TEST_IMAGES_flickr8k_5_cap_per_img_5_min_word_freq.hdf5 (deflated 17%)\n",
            "  adding: caption_data_flickr8k/TEST_CAPLENS_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 79%)\n",
            "  adding: caption_data_flickr8k/TRAIN_CAPLENS_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 80%)\n",
            "  adding: caption_data_flickr8k/VAL_CAPLENS_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 79%)\n",
            "  adding: caption_data_flickr8k/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 61%)\n",
            "  adding: caption_data_flickr8k/VAL_CAPTIONS_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 91%)\n",
            "  adding: caption_data_flickr8k/TEST_CAPTIONS_flickr8k_5_cap_per_img_5_min_word_freq.json (deflated 91%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCT4Eu6julTY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyKRmMg1vRKf"
      },
      "source": [
        "! cp caption_data_flickr8k.zip /content/drive/MyDrive/EVA4P2/12-ImageCaptioning/"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcPSFRFQvXzc"
      },
      "source": [
        "! cp BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar /content/drive/MyDrive/EVA4P2/12-ImageCaptioning/"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogOwQJRswwC0"
      },
      "source": [
        "! cp BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq_state_dict.pt /content/drive/MyDrive/EVA4P2/12-ImageCaptioning/"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb2NaFqWveF8"
      },
      "source": [
        "! cp caption_data_flickr8k/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json /content/drive/MyDrive/EVA4P2/12-ImageCaptioning/"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvCDWO5wWqX"
      },
      "source": [
        "## Save the models as script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxxZFw-KweRR"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "class DecoderWithAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_dim,\n",
        "        embed_dim,\n",
        "        decoder_dim,\n",
        "        vocab_size,\n",
        "        encoder_dim=2048,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention2, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(\n",
        "            encoder_dim, decoder_dim, attention_dim\n",
        "        )  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(\n",
        "            embed_dim + encoder_dim, decoder_dim, bias=True\n",
        "        )  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(\n",
        "            encoder_dim, decoder_dim\n",
        "        )  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(\n",
        "            decoder_dim, encoder_dim\n",
        "        )  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(\n",
        "            decoder_dim, vocab_size\n",
        "        )  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(\n",
        "            batch_size, -1, encoder_dim\n",
        "        )  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(\n",
        "            dim=0, descending=True\n",
        "        )\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(\n",
        "            encoded_captions\n",
        "        )  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths: List[int] = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = torch.sum(torch.tensor([l > t for l in decode_lengths])).item()\n",
        "            attention_weighted_encoding, alpha = self.attention(\n",
        "                encoder_out[:batch_size_t], h[:batch_size_t]\n",
        "            )\n",
        "            gate = self.sigmoid(\n",
        "                self.f_beta(h[:batch_size_t])\n",
        "            )  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat(\n",
        "                    [embeddings[:batch_size_t, t, :], attention_weighted_encoding],\n",
        "                    dim=1,\n",
        "                ),\n",
        "                (h[:batch_size_t], c[:batch_size_t]),\n",
        "            )  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg9lIkrCwj_c"
      },
      "source": [
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"decoder\": decoder.state_dict()\n",
        "}, \"BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq_state_dict.pt\")"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj-1wb6UvsM7"
      },
      "source": [
        "encoder_script = torch.jit.script(encoder.to(\"cpu\"))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkkiDYemxLev"
      },
      "source": [
        "decoder2 = DecoderWithAttention2(\n",
        "        attention_dim=attention_dim,\n",
        "        embed_dim=emb_dim,\n",
        "        decoder_dim=decoder_dim,\n",
        "        vocab_size=len(word_map),\n",
        "        dropout=dropout,\n",
        "        encoder_dim=encoder_dim\n",
        "    ).to(\"cpu\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOObfkrEzA0y",
        "outputId": "a3d6c166-e6ff-48a1-fa65-9ee08bc103f4"
      },
      "source": [
        "decoder2.load_state_dict(decoder.state_dict())"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cZ2u9vxxT-u"
      },
      "source": [
        "decoder2_script = torch.jit.script(decoder2.to(\"cpu\"))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DfzqF8rzcvB"
      },
      "source": [
        "encoder_script.save(\"flickr8k_caption.encoder.scripted.pt\")"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVLwG3uvzsX3"
      },
      "source": [
        "decoder2_script.save(\"flickr8k_caption.decoder.scripted.pt\")"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qGD_EjMzuy6"
      },
      "source": [
        "! cp flickr8k_caption.encoder.scripted.pt /content/drive/MyDrive/EVA4P2/12-ImageCaptioning/"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS6NKplS0K8L"
      },
      "source": [
        "! cp flickr8k_caption.decoder.scripted.pt /content/drive/MyDrive/EVA4P2/12-ImageCaptioning/"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULG1wdN90Nfn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}