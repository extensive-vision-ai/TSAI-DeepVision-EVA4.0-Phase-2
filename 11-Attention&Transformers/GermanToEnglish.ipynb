{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GermanToEnglish.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfhWKXtwKlgzF+Hm1g3ocj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepVision-EVA4.0-Phase-2/blob/master/11-Attention%26Transformers/GermanToEnglish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zotEcShghMvX"
      },
      "source": [
        "# German to English Translator using Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxR90GNmhJ9f",
        "outputId": "54b779c0-5824-4244-e595-24492a9d3cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov  1 15:02:55 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4QyWXR9d338",
        "outputId": "24077a61-de4c-4981-8515-8d95fdc7fe2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! python -m spacy download en\n",
        "! python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 557kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.1)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=d100c7ed88db20bfd8b2b1fb968871830f91fbee04859b4430e3fbd4b9d0c932\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tcnwe2_n/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sUo5T9_gBU-"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkPr4QP-dpdj"
      },
      "source": [
        "from torchtext import data, datasets\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from IPython.core.debugger import set_trace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuSw9c1bgHSW",
        "outputId": "2419bd13-1bff-451a-80e1-1f1b43a46ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we will use CUDA if it is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # or set to 'cpu'\n",
        "print(\"CUDA:\", USE_CUDA)\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA: True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyrihE6Ael8x"
      },
      "source": [
        "# Let's start coding!\n",
        "\n",
        "## Model class\n",
        "\n",
        "Our base model class `EncoderDecoder` is very similar to the one in *The Annotated Transformer*.\n",
        "\n",
        "One difference is that our encoder also returns its final states (`encoder_final` below), which is used to initialize the decoder RNN. We also provide the sequence lengths as the RNNs require those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqyhhv3Fel8x"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
        "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
        "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
        "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask, src_lengths):\n",
        "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
        "    \n",
        "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
        "               decoder_hidden=None):\n",
        "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
        "                            src_mask, trg_mask, hidden=decoder_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFHpSpPHel80"
      },
      "source": [
        "To keep things easy we also keep the `Generator` class the same. \n",
        "It simply projects the pre-output layer ($x$ in the `forward` function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBHlik8sel80"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TRicnL4el82"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "Our encoder is a bi-directional GRU. \n",
        "\n",
        "Because we want to process multiple sentences at the same time for speed reasons (it is more effcient on GPU), we need to support **mini-batches**. Sentences in a mini-batch may have different lengths, which means that the RNN needs to unroll further for certain sentences while it might already have finished for others:\n",
        "\n",
        "```\n",
        "Example: mini-batch with 3 source sentences of different lengths (7, 5, and 3).\n",
        "End-of-sequence is marked with a \"3\" here, and padding positions with \"1\".\n",
        "\n",
        "+---------------+\n",
        "| 4 5 9 8 7 8 3 |\n",
        "+---------------+\n",
        "| 5 4 8 7 3 1 1 |\n",
        "+---------------+\n",
        "| 5 8 3 1 1 1 1 |\n",
        "+---------------+\n",
        "```\n",
        "You can see that, when computing hidden states for this mini-batch, for sentence #2 and #3 we will need to stop updating the hidden state after we have encountered \"3\". We don't want to incorporate the padding values (1s).\n",
        "\n",
        "Luckily, PyTorch has convenient helper functions called `pack_padded_sequence` and `pad_packed_sequence`.\n",
        "These functions take care of masking and padding, so that the resulting word representations are simply zeros after a sentence stops.\n",
        "\n",
        "The code below reads in a source sentence (a sequence of word embeddings) and produces the hidden states.\n",
        "It also returns a final vector, a summary of the complete sentence, by concatenating the first and the last hidden states (they have both seen the whole sentence, each in a different direction). We will use the final vector to initialize the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQDGL4cEel83"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
        "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Applies a bidirectional GRU to sequence of embeddings x.\n",
        "        The input mini-batch x needs to be sorted by length.\n",
        "        x should have dimensions [batch, time, dim].\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        output, final = self.rnn(packed)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # we need to manually concatenate the final states for both directions\n",
        "        fwd_final = final[0:final.size(0):2]\n",
        "        bwd_final = final[1:final.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        return output, final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj8ZLW9rel86"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder is a conditional GRU. Rather than starting with an empty state like the encoder, its initial hidden state results from a projection of the encoder final vector. \n",
        "\n",
        "#### Training\n",
        "In `forward` you can find a for-loop that computes the decoder hidden states one time step at a time. \n",
        "Note that, during training, we know exactly what the target words should be! (They are in `trg_embed`.) This means that we are not even checking here what the prediction is! We simply feed the correct previous target word embedding to the GRU at each time step. This is called teacher forcing.\n",
        "\n",
        "The `forward` function returns all decoder hidden states and pre-output vectors. Elsewhere these are used to compute the loss, after which the parameters are updated.\n",
        "\n",
        "#### Prediction\n",
        "For prediction time, for forward function is only used for a single time step. After predicting a word from the returned pre-output vector, we can call it again, supplying it the word embedding of the previously predicted word and the last state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5BTFfPqel86"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
        "                 bridge=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "                 \n",
        "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "                 \n",
        "        # to initialize from the final encoder state\n",
        "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
        "                                          hidden_size, bias=False)\n",
        "        \n",
        "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
        "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, proj_key=proj_key,\n",
        "            value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "        # update rnn hidden state\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        \n",
        "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "        pre_output = self.dropout_layer(pre_output)\n",
        "        pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "        return output, hidden, pre_output\n",
        "    \n",
        "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
        "                src_mask, trg_mask, hidden=None, max_len=None):\n",
        "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
        "                                         \n",
        "        # the maximum number of steps to unroll the RNN\n",
        "        if max_len is None:\n",
        "            max_len = trg_mask.size(-1)\n",
        "\n",
        "        # initialize decoder hidden state\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(encoder_final)\n",
        "        \n",
        "        # pre-compute projected encoder hidden states\n",
        "        # (the \"keys\" for the attention mechanism)\n",
        "        # this is only done for efficiency\n",
        "        proj_key = self.attention.key_layer(encoder_hidden)\n",
        "        \n",
        "        # here we store all intermediate hidden states and pre-output vectors\n",
        "        decoder_states = []\n",
        "        pre_output_vectors = []\n",
        "        \n",
        "        # unroll the decoder RNN for max_len steps\n",
        "        for i in range(max_len):\n",
        "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
        "            output, hidden, pre_output = self.forward_step(\n",
        "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
        "            decoder_states.append(output)\n",
        "            pre_output_vectors.append(pre_output)\n",
        "\n",
        "        decoder_states = torch.cat(decoder_states, dim=1)\n",
        "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
        "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
        "\n",
        "    def init_hidden(self, encoder_final):\n",
        "        \"\"\"Returns the initial decoder state,\n",
        "        conditioned on the final encoder state.\"\"\"\n",
        "\n",
        "        if encoder_final is None:\n",
        "            return None  # start with zeros\n",
        "\n",
        "        return torch.tanh(self.bridge(encoder_final))            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yehvAId2el8-"
      },
      "source": [
        "### Attention                                                                                                                                                                               \n",
        "\n",
        "At every time step, the decoder has access to *all* source word representations $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$. \n",
        "An attention mechanism allows the model to focus on the currently most relevant part of the source sentence.\n",
        "The state of the decoder is represented by GRU hidden state $\\mathbf{s}_i$.\n",
        "So if we want to know which source word representation(s) $\\mathbf{h}_j$ are most relevant, we will need to define a function that takes those two things as input.\n",
        "\n",
        "Here we use the MLP-based, additive attention that was used in Bahdanau et al.:\n",
        "\n",
        "<img src=\"https://github.com/bastings/annotated_encoder_decoder/blob/master/images/attention.png?raw=1\" width=\"280\">\n",
        "\n",
        "\n",
        "We apply an MLP with tanh-activation to both the current decoder state $\\bf s_i$ (the *query*) and each encoder state $\\bf h_j$ (the *key*), and then project this to a single value (i.e. a scalar) to get the *attention energy* $e_{ij}$. \n",
        "\n",
        "Once all energies are computed, they are normalized by a softmax so that they sum to one: \n",
        "\n",
        "$$ \\alpha_{ij} = \\text{softmax}(\\mathbf{e}_i)[j] $$\n",
        "\n",
        "$$\\sum_j \\alpha_{ij} = 1.0$$ \n",
        "\n",
        "The context vector for time step $i$ is then a weighted sum of the encoder hidden states (the *values*):\n",
        "$$\\mathbf{c}_i = \\sum_j \\alpha_{ij} \\mathbf{h}_j$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbfVoTcbel8-"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
        "        key_size = 2 * hidden_size if key_size is None else key_size\n",
        "        query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "        \n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "        assert mask is not None, \"mask is required\"\n",
        "\n",
        "        # We first project the query (the decoder state).\n",
        "        # The projected keys (the encoder states) were already pre-computated.\n",
        "        query = self.query_layer(query)\n",
        "        \n",
        "        # Calculate scores.\n",
        "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        \n",
        "        # Mask out invalid positions.\n",
        "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "        \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=-1)\n",
        "        self.alphas = alphas        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        context = torch.bmm(alphas, value)\n",
        "        \n",
        "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "        return context, alphas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbWDL3wel9B"
      },
      "source": [
        "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
        "We use learned embeddings to convert the input tokens and output tokens to vectors of dimension `emb_size`.\n",
        "\n",
        "We will simply use PyTorch's [nn.Embedding](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOVGVz5Wel9C"
      },
      "source": [
        "## Full Model\n",
        "\n",
        "Here we define a function from hyperparameters to a full model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtGXFN2Oel9C"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "\n",
        "    attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
        "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
        "        nn.Embedding(src_vocab, emb_size),\n",
        "        nn.Embedding(tgt_vocab, emb_size),\n",
        "        Generator(hidden_size, tgt_vocab))\n",
        "\n",
        "    return model.cuda() if USE_CUDA else model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpAsDvsel9S"
      },
      "source": [
        "## Loss Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEwL1gG8el9S"
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                              y.contiguous().view(-1))\n",
        "        loss = loss / norm\n",
        "\n",
        "        if self.opt is not None:\n",
        "            loss.backward()          \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        return loss.data.item() * norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnGuByYNel9U"
      },
      "source": [
        "### Printing examples\n",
        "\n",
        "To monitor progress during training, we will translate a few examples.\n",
        "\n",
        "We use greedy decoding for simplicity; that is, at each time step, starting at the first token, we choose the one with that maximum probability, and we never revisit that choice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKQ2BRoKel9U"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
        "    \"\"\"Greedily decode a sentence.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
        "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
        "        trg_mask = torch.ones_like(prev_y)\n",
        "\n",
        "    output = []\n",
        "    attention_scores = []\n",
        "    hidden = None\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out, hidden, pre_output = model.decode(\n",
        "              encoder_hidden, encoder_final, src_mask,\n",
        "              prev_y, trg_mask, hidden)\n",
        "\n",
        "            # we predict from the pre-output layer, which is\n",
        "            # a combination of Decoder state, prev emb, and context\n",
        "            prob = model.generator(pre_output[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data.item()\n",
        "        output.append(next_word)\n",
        "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
        "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
        "    \n",
        "    output = np.array(output)\n",
        "        \n",
        "    # cut off everything starting from </s> \n",
        "    # (only when eos_index provided)\n",
        "    if eos_index is not None:\n",
        "        first_eos = np.where(output==eos_index)[0]\n",
        "        if len(first_eos) > 0:\n",
        "            output = output[:first_eos[0]]      \n",
        "    \n",
        "    return output, np.concatenate(attention_scores, axis=1)\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab=None):\n",
        "    if vocab is not None:\n",
        "        x = [vocab.itos[i] for i in x]\n",
        "\n",
        "    return [str(t) for t in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEA0ynFvel9W"
      },
      "source": [
        "def print_examples(example_iter, model, n=2, max_len=100, \n",
        "                   sos_index=1, \n",
        "                   src_eos_index=None, \n",
        "                   trg_eos_index=None, \n",
        "                   src_vocab=None, trg_vocab=None):\n",
        "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    print()\n",
        "    \n",
        "    if src_vocab is not None and trg_vocab is not None:\n",
        "        src_eos_index = src_vocab.stoi[EOS_TOKEN]\n",
        "        trg_sos_index = trg_vocab.stoi[SOS_TOKEN]\n",
        "        trg_eos_index = trg_vocab.stoi[EOS_TOKEN]\n",
        "    else:\n",
        "        src_eos_index = None\n",
        "        trg_sos_index = 1\n",
        "        trg_eos_index = None\n",
        "        \n",
        "    for i, batch in enumerate(example_iter):\n",
        "      \n",
        "        src = batch.src.cpu().numpy()[0, :]\n",
        "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
        "\n",
        "        # remove </s> (if it is there)\n",
        "        src = src[:-1] if src[-1] == src_eos_index else src\n",
        "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
        "      \n",
        "        result, _ = greedy_decode(\n",
        "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
        "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
        "        print(\"Example #%d\" % (i+1))\n",
        "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
        "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
        "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
        "        print()\n",
        "        \n",
        "        count += 1\n",
        "        if count == n:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz_B-snfel9E"
      },
      "source": [
        "# Training\n",
        "\n",
        "This section describes the training regime for our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrH24wXSel9F"
      },
      "source": [
        "We stop for a quick interlude to introduce some of the tools \n",
        "needed to train a standard encoder decoder model. First we define a batch object that holds the src and target sentences for training, as well as their lengths and masks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIdOhyzbel9F"
      },
      "source": [
        "## Batches and Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uj4wW5sel9G"
      },
      "source": [
        "class Batch:\n",
        "    \"\"\"Object for holding a batch of data with mask during training.\n",
        "    Input is a batch from a torch text iterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, trg, pad_index=0):\n",
        "        \n",
        "        src, src_lengths = src\n",
        "        \n",
        "        self.src = src\n",
        "        self.src_lengths = src_lengths\n",
        "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
        "        self.nseqs = src.size(0)\n",
        "        \n",
        "        self.trg = None\n",
        "        self.trg_y = None\n",
        "        self.trg_mask = None\n",
        "        self.trg_lengths = None\n",
        "        self.ntokens = None\n",
        "\n",
        "        if trg is not None:\n",
        "            trg, trg_lengths = trg\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_lengths = trg_lengths\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = (self.trg_y != pad_index)\n",
        "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            self.src = self.src.cuda()\n",
        "            self.src_mask = self.src_mask.cuda()\n",
        "\n",
        "            if trg is not None:\n",
        "                self.trg = self.trg.cuda()\n",
        "                self.trg_y = self.trg_y.cuda()\n",
        "                self.trg_mask = self.trg_mask.cuda()\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJRrLx5Wel9K"
      },
      "source": [
        "## Training Loop\n",
        "The code below trains the model for 1 epoch (=1 pass through the training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNULic00el9L"
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
        "    \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    print_tokens = 0\n",
        "\n",
        "    for i, batch in enumerate(data_iter, 1):\n",
        "        \n",
        "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
        "                                           batch.src_mask, batch.trg_mask,\n",
        "                                           batch.src_lengths, batch.trg_lengths)\n",
        "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        print_tokens += batch.ntokens\n",
        "        \n",
        "        if model.training and i % print_every == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
        "            start = time.time()\n",
        "            print_tokens = 0\n",
        "\n",
        "    return math.exp(total_loss / float(total_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MYb-wo6f-3Q"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDDVyhu1eFQH"
      },
      "source": [
        "Load the German and the English Spacy Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu_W4TwefyF2"
      },
      "source": [
        "Read about the IWSLT dataset here: https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/datasets/iwslt.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb6JBcIgdwD4",
        "outputId": "e8f1950d-fe2e-4aab-ee50-e92dbb87a7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_TOKEN = \"<pad>\"    \n",
        "SOS_TOKEN = \"<s>\"\n",
        "EOS_TOKEN = \"</s>\"\n",
        "LOWER = True\n",
        "\n",
        "# we include lengths to provide to the RNNs\n",
        "SRC = data.Field(tokenize=tokenize_de, \n",
        "                    batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                    unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=None, eos_token=EOS_TOKEN)\n",
        "TRG = data.Field(tokenize=tokenize_en, \n",
        "                    batch_first=True, lower=LOWER, include_lengths=True,\n",
        "                    unk_token=UNK_TOKEN, pad_token=PAD_TOKEN, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
        "\n",
        "MAX_LEN = 25  # NOTE: we filter out a lot of sentences for speed\n",
        "train_data, valid_data, test_data = datasets.IWSLT.splits(\n",
        "    exts=('.de', '.en'), fields=(SRC, TRG), \n",
        "    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "        len(vars(x)['trg']) <= MAX_LEN)\n",
        "MIN_FREQ = 5  # NOTE: we limit the vocabulary to frequent words for speed\n",
        "SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
        "TRG.build_vocab(train_data.trg, min_freq=MIN_FREQ)\n",
        "\n",
        "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:07<00:00, 3.18MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1MCW7XpeMGY"
      },
      "source": [
        "It never hurts to look at your data and some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suRrFoDPd0Bo",
        "outputId": "c7261f7c-26e3-410d-eb9b-d3ee83ffe8ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
        "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
        "\n",
        "    print(\"Data set sizes (number of sentence pairs):\")\n",
        "    print('train', len(train_data))\n",
        "    print('valid', len(valid_data))\n",
        "    print('test', len(test_data), \"\\n\")\n",
        "\n",
        "    print(\"First training example:\")\n",
        "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
        "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
        "\n",
        "    print(\"Most common words (src):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "    print(\"Most common words (trg):\")\n",
        "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
        "\n",
        "    print(\"First 10 words (src):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
        "    print(\"First 10 words (trg):\")\n",
        "    print(\"\\n\".join(\n",
        "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
        "\n",
        "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
        "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\")\n",
        "    \n",
        "    \n",
        "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data set sizes (number of sentence pairs):\n",
            "train 143115\n",
            "valid 690\n",
            "test 963 \n",
            "\n",
            "First training example:\n",
            "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
            "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
            "\n",
            "Most common words (src):\n",
            "         .     138329\n",
            "         ,     105944\n",
            "       und      41843\n",
            "       die      40808\n",
            "       das      33324\n",
            "       sie      33034\n",
            "       ich      31150\n",
            "       ist      31037\n",
            "        es      27449\n",
            "       wir      25817 \n",
            "\n",
            "Most common words (trg):\n",
            "         .     137259\n",
            "         ,      91615\n",
            "       the      73343\n",
            "       and      50276\n",
            "        to      42799\n",
            "         a      39572\n",
            "        of      39496\n",
            "         i      33521\n",
            "        it      32920\n",
            "      that      32640 \n",
            "\n",
            "First 10 words (src):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 </s>\n",
            "03 .\n",
            "04 ,\n",
            "05 und\n",
            "06 die\n",
            "07 das\n",
            "08 sie\n",
            "09 ich \n",
            "\n",
            "First 10 words (trg):\n",
            "00 <unk>\n",
            "01 <pad>\n",
            "02 <s>\n",
            "03 </s>\n",
            "04 .\n",
            "05 ,\n",
            "06 the\n",
            "07 and\n",
            "08 to\n",
            "09 a \n",
            "\n",
            "Number of German words (types): 15765\n",
            "Number of English words (types): 13002 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QslcmDZf5fX"
      },
      "source": [
        "## Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lNHZDnFeUW6"
      },
      "source": [
        "train_iter = data.BucketIterator(train_data, batch_size=64, train=True, \n",
        "                                 sort_within_batch=True, \n",
        "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
        "                                 device=DEVICE)\n",
        "valid_iter = data.Iterator(valid_data, batch_size=1, train=False, sort=False, repeat=False, \n",
        "                           device=DEVICE)\n",
        "\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
        "    return Batch(batch.src, batch.trg, pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvIFYLsvgWeX"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12vewsE5f4qj"
      },
      "source": [
        "def train(model, num_epochs=10, lr=0.0003, print_every=100):\n",
        "    \"\"\"Train a model on IWSLT\"\"\"\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "\n",
        "    # optionally add label smoothing; see the Annotated Transformer\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    dev_perplexities = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        print(\"Epoch\", epoch)\n",
        "        model.train()\n",
        "        train_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in train_iter), \n",
        "                                     model,\n",
        "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
        "                                     print_every=print_every)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            print_examples((rebatch(PAD_INDEX, x) for x in valid_iter), \n",
        "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
        "\n",
        "            dev_perplexity = run_epoch((rebatch(PAD_INDEX, b) for b in valid_iter), \n",
        "                                       model, \n",
        "                                       SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
        "            dev_perplexities.append(dev_perplexity)\n",
        "        \n",
        "    return dev_perplexities\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mkW9DEwgXvK",
        "outputId": "a8c2b28a-3ff9-459a-a8d2-677e84ddb520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)\n",
        "\n",
        "dev_perplexities = train(model, print_every=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch Step: 100 Loss: 47.489826 Tokens per Sec: 17749.757957\n",
            "Epoch Step: 200 Loss: 61.986183 Tokens per Sec: 19837.206253\n",
            "Epoch Step: 300 Loss: 39.708763 Tokens per Sec: 19708.884550\n",
            "Epoch Step: 400 Loss: 24.749378 Tokens per Sec: 20252.456148\n",
            "Epoch Step: 500 Loss: 30.439274 Tokens per Sec: 19740.000241\n",
            "Epoch Step: 600 Loss: 91.155182 Tokens per Sec: 19880.926094\n",
            "Epoch Step: 700 Loss: 29.596691 Tokens per Sec: 19769.821996\n",
            "Epoch Step: 800 Loss: 27.381372 Tokens per Sec: 19743.304746\n",
            "Epoch Step: 900 Loss: 62.975502 Tokens per Sec: 19881.637328\n",
            "Epoch Step: 1000 Loss: 15.430821 Tokens per Sec: 19719.260159\n",
            "Epoch Step: 1100 Loss: 56.118473 Tokens per Sec: 19515.405757\n",
            "Epoch Step: 1200 Loss: 44.520882 Tokens per Sec: 19875.606351\n",
            "Epoch Step: 1300 Loss: 75.150215 Tokens per Sec: 20122.929387\n",
            "Epoch Step: 1400 Loss: 97.098038 Tokens per Sec: 19886.538729\n",
            "Epoch Step: 1500 Loss: 32.125546 Tokens per Sec: 20104.474083\n",
            "Epoch Step: 1600 Loss: 45.931583 Tokens per Sec: 20044.875999\n",
            "Epoch Step: 1700 Loss: 78.498787 Tokens per Sec: 19510.773495\n",
            "Epoch Step: 1800 Loss: 78.987282 Tokens per Sec: 19623.508316\n",
            "Epoch Step: 1900 Loss: 42.520630 Tokens per Sec: 19988.858008\n",
            "Epoch Step: 2000 Loss: 68.699661 Tokens per Sec: 19454.231990\n",
            "Epoch Step: 2100 Loss: 91.994301 Tokens per Sec: 19343.274035\n",
            "Epoch Step: 2200 Loss: 30.275684 Tokens per Sec: 19712.152351\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was five years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on the <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he 's very much , which was pretty good , because it was a <unk> that was the <unk> of the <unk> .\n",
            "\n",
            "Validation perplexity: 31.966158\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 17.660383 Tokens per Sec: 18782.627160\n",
            "Epoch Step: 200 Loss: 65.983757 Tokens per Sec: 19787.629992\n",
            "Epoch Step: 300 Loss: 51.701958 Tokens per Sec: 19716.849435\n",
            "Epoch Step: 400 Loss: 42.760544 Tokens per Sec: 20106.799321\n",
            "Epoch Step: 500 Loss: 60.421017 Tokens per Sec: 20009.538035\n",
            "Epoch Step: 600 Loss: 18.480532 Tokens per Sec: 19807.297096\n",
            "Epoch Step: 700 Loss: 36.231167 Tokens per Sec: 19713.504819\n",
            "Epoch Step: 800 Loss: 48.313198 Tokens per Sec: 19623.481308\n",
            "Epoch Step: 900 Loss: 22.455109 Tokens per Sec: 19454.948826\n",
            "Epoch Step: 1000 Loss: 41.783772 Tokens per Sec: 19849.826545\n",
            "Epoch Step: 1100 Loss: 48.169193 Tokens per Sec: 19770.550781\n",
            "Epoch Step: 1200 Loss: 53.910652 Tokens per Sec: 19700.637293\n",
            "Epoch Step: 1300 Loss: 36.775410 Tokens per Sec: 19703.526100\n",
            "Epoch Step: 1400 Loss: 69.729576 Tokens per Sec: 19789.023404\n",
            "Epoch Step: 1500 Loss: 80.231903 Tokens per Sec: 19530.988951\n",
            "Epoch Step: 1600 Loss: 62.056381 Tokens per Sec: 19634.546499\n",
            "Epoch Step: 1700 Loss: 13.059239 Tokens per Sec: 19606.530790\n",
            "Epoch Step: 1800 Loss: 26.123343 Tokens per Sec: 19924.302960\n",
            "Epoch Step: 1900 Loss: 31.710855 Tokens per Sec: 19917.041200\n",
            "Epoch Step: 2000 Loss: 48.360748 Tokens per Sec: 19670.885617\n",
            "Epoch Step: 2100 Loss: 25.848534 Tokens per Sec: 19582.978322\n",
            "Epoch Step: 2200 Loss: 31.127417 Tokens per Sec: 19661.307654\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , the <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , what was pretty fascinating was , because it was the <unk> .\n",
            "\n",
            "Validation perplexity: 19.991604\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 15.363278 Tokens per Sec: 18856.520084\n",
            "Epoch Step: 200 Loss: 41.980076 Tokens per Sec: 19525.616083\n",
            "Epoch Step: 300 Loss: 15.045265 Tokens per Sec: 19628.687843\n",
            "Epoch Step: 400 Loss: 42.359818 Tokens per Sec: 19854.848966\n",
            "Epoch Step: 500 Loss: 65.769539 Tokens per Sec: 19882.447822\n",
            "Epoch Step: 600 Loss: 40.152237 Tokens per Sec: 19783.642204\n",
            "Epoch Step: 700 Loss: 31.274982 Tokens per Sec: 20045.099917\n",
            "Epoch Step: 800 Loss: 48.950188 Tokens per Sec: 19733.015309\n",
            "Epoch Step: 900 Loss: 63.142349 Tokens per Sec: 19633.672398\n",
            "Epoch Step: 1000 Loss: 23.315367 Tokens per Sec: 19910.352140\n",
            "Epoch Step: 1100 Loss: 15.675964 Tokens per Sec: 19731.990298\n",
            "Epoch Step: 1200 Loss: 40.949913 Tokens per Sec: 20262.508492\n",
            "Epoch Step: 1300 Loss: 17.080482 Tokens per Sec: 19916.563718\n",
            "Epoch Step: 1400 Loss: 78.750542 Tokens per Sec: 19881.478542\n",
            "Epoch Step: 1500 Loss: 31.194683 Tokens per Sec: 20191.228679\n",
            "Epoch Step: 1600 Loss: 71.822403 Tokens per Sec: 19851.337674\n",
            "Epoch Step: 1700 Loss: 11.235020 Tokens per Sec: 19836.935042\n",
            "Epoch Step: 1800 Loss: 29.048222 Tokens per Sec: 20056.530120\n",
            "Epoch Step: 1900 Loss: 74.858704 Tokens per Sec: 19903.393865\n",
            "Epoch Step: 2000 Loss: 25.598612 Tokens per Sec: 19840.386380\n",
            "Epoch Step: 2100 Loss: 61.814304 Tokens per Sec: 20170.419582\n",
            "Epoch Step: 2200 Loss: 13.937226 Tokens per Sec: 20444.915273\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little , <unk> , the <unk> of the <unk> .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a very little bit of what was happening in the same , because it was the news to the <unk> .\n",
            "\n",
            "Validation perplexity: 15.421709\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 56.380241 Tokens per Sec: 19417.674823\n",
            "Epoch Step: 200 Loss: 16.517548 Tokens per Sec: 19696.183390\n",
            "Epoch Step: 300 Loss: 64.248276 Tokens per Sec: 20315.353740\n",
            "Epoch Step: 400 Loss: 61.940643 Tokens per Sec: 19985.134015\n",
            "Epoch Step: 500 Loss: 48.531860 Tokens per Sec: 20135.449111\n",
            "Epoch Step: 600 Loss: 55.569160 Tokens per Sec: 19796.341736\n",
            "Epoch Step: 700 Loss: 21.607695 Tokens per Sec: 20065.901669\n",
            "Epoch Step: 800 Loss: 60.758495 Tokens per Sec: 20091.442257\n",
            "Epoch Step: 900 Loss: 65.969337 Tokens per Sec: 20043.572558\n",
            "Epoch Step: 1000 Loss: 10.653299 Tokens per Sec: 20253.503797\n",
            "Epoch Step: 1100 Loss: 42.165710 Tokens per Sec: 20308.938556\n",
            "Epoch Step: 1200 Loss: 40.668289 Tokens per Sec: 19670.240505\n",
            "Epoch Step: 1300 Loss: 31.035696 Tokens per Sec: 19951.663055\n",
            "Epoch Step: 1400 Loss: 38.577908 Tokens per Sec: 19624.546475\n",
            "Epoch Step: 1500 Loss: 41.721809 Tokens per Sec: 20003.454581\n",
            "Epoch Step: 1600 Loss: 52.885685 Tokens per Sec: 20216.217054\n",
            "Epoch Step: 1700 Loss: 10.841859 Tokens per Sec: 20003.543025\n",
            "Epoch Step: 1800 Loss: 19.404921 Tokens per Sec: 19921.735649\n",
            "Epoch Step: 1900 Loss: 22.845886 Tokens per Sec: 19741.582050\n",
            "Epoch Step: 2000 Loss: 52.290161 Tokens per Sec: 19741.401808\n",
            "Epoch Step: 2100 Loss: 9.151881 Tokens per Sec: 20079.762376\n",
            "Epoch Step: 2200 Loss: 53.152306 Tokens per Sec: 19851.039462\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was born by a morning of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was sitting on his little , <unk> radio the <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a very happy out of what happened was going to see the news <unk> .\n",
            "\n",
            "Validation perplexity: 13.777755\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 20.936640 Tokens per Sec: 19426.227018\n",
            "Epoch Step: 200 Loss: 28.832478 Tokens per Sec: 19665.424338\n",
            "Epoch Step: 300 Loss: 52.483807 Tokens per Sec: 19782.017167\n",
            "Epoch Step: 400 Loss: 39.327896 Tokens per Sec: 19677.605069\n",
            "Epoch Step: 500 Loss: 15.517988 Tokens per Sec: 20047.695845\n",
            "Epoch Step: 600 Loss: 44.175461 Tokens per Sec: 19941.595544\n",
            "Epoch Step: 700 Loss: 38.195625 Tokens per Sec: 19340.100544\n",
            "Epoch Step: 800 Loss: 67.665337 Tokens per Sec: 19745.160133\n",
            "Epoch Step: 900 Loss: 39.206425 Tokens per Sec: 19925.430141\n",
            "Epoch Step: 1000 Loss: 12.341382 Tokens per Sec: 20108.010340\n",
            "Epoch Step: 1100 Loss: 36.264755 Tokens per Sec: 20047.228740\n",
            "Epoch Step: 1200 Loss: 31.666981 Tokens per Sec: 19953.089233\n",
            "Epoch Step: 1300 Loss: 36.322697 Tokens per Sec: 19967.327660\n",
            "Epoch Step: 1400 Loss: 31.503513 Tokens per Sec: 19984.260227\n",
            "Epoch Step: 1500 Loss: 29.395420 Tokens per Sec: 20086.948222\n",
            "Epoch Step: 1600 Loss: 37.207973 Tokens per Sec: 20084.708845\n",
            "Epoch Step: 1700 Loss: 60.045433 Tokens per Sec: 19989.431733\n",
            "Epoch Step: 1800 Loss: 34.609081 Tokens per Sec: 20061.544004\n",
            "Epoch Step: 1900 Loss: 19.654558 Tokens per Sec: 19880.046066\n",
            "Epoch Step: 2000 Loss: 15.826859 Tokens per Sec: 20024.112940\n",
            "Epoch Step: 2100 Loss: 32.549099 Tokens per Sec: 19985.289016\n",
            "Epoch Step: 2200 Loss: 19.265366 Tokens per Sec: 20274.729163\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of joy .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my dad heard on his little , <unk> radio the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a very happy , which was pretty unusual , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 12.569031\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 59.438576 Tokens per Sec: 19069.493187\n",
            "Epoch Step: 200 Loss: 16.759537 Tokens per Sec: 19880.446819\n",
            "Epoch Step: 300 Loss: 55.878078 Tokens per Sec: 20033.130662\n",
            "Epoch Step: 400 Loss: 17.429644 Tokens per Sec: 19860.840400\n",
            "Epoch Step: 500 Loss: 50.873905 Tokens per Sec: 19547.673157\n",
            "Epoch Step: 600 Loss: 26.600210 Tokens per Sec: 20270.200519\n",
            "Epoch Step: 700 Loss: 32.668049 Tokens per Sec: 19944.175675\n",
            "Epoch Step: 800 Loss: 26.260855 Tokens per Sec: 20092.743225\n",
            "Epoch Step: 900 Loss: 38.793213 Tokens per Sec: 20160.045245\n",
            "Epoch Step: 1000 Loss: 41.772137 Tokens per Sec: 20136.923841\n",
            "Epoch Step: 1100 Loss: 37.113514 Tokens per Sec: 20121.921228\n",
            "Epoch Step: 1200 Loss: 32.034443 Tokens per Sec: 19916.278015\n",
            "Epoch Step: 1300 Loss: 11.940507 Tokens per Sec: 19841.753281\n",
            "Epoch Step: 1400 Loss: 20.691677 Tokens per Sec: 20099.330414\n",
            "Epoch Step: 1500 Loss: 57.370213 Tokens per Sec: 20183.886677\n",
            "Epoch Step: 1600 Loss: 27.803143 Tokens per Sec: 20241.923040\n",
            "Epoch Step: 1700 Loss: 10.210357 Tokens per Sec: 20017.211500\n",
            "Epoch Step: 1800 Loss: 34.586914 Tokens per Sec: 20152.268300\n",
            "Epoch Step: 1900 Loss: 37.037262 Tokens per Sec: 20097.230353\n",
            "Epoch Step: 2000 Loss: 49.456409 Tokens per Sec: 19945.650097\n",
            "Epoch Step: 2100 Loss: 21.512598 Tokens per Sec: 20163.827059\n",
            "Epoch Step: 2200 Loss: 33.312077 Tokens per Sec: 20133.054938\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a <unk> of the <unk> of the <unk> of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was walking on his little boy , the radio shack of the bbc 's bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was the <unk> .\n",
            "\n",
            "Validation perplexity: 12.124049\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 17.389519 Tokens per Sec: 19293.497229\n",
            "Epoch Step: 200 Loss: 28.280071 Tokens per Sec: 19818.911297\n",
            "Epoch Step: 300 Loss: 29.418669 Tokens per Sec: 19937.279334\n",
            "Epoch Step: 400 Loss: 30.130220 Tokens per Sec: 19543.257746\n",
            "Epoch Step: 500 Loss: 60.150303 Tokens per Sec: 19694.168078\n",
            "Epoch Step: 600 Loss: 43.765728 Tokens per Sec: 19689.356319\n",
            "Epoch Step: 700 Loss: 27.697321 Tokens per Sec: 20074.050391\n",
            "Epoch Step: 800 Loss: 23.965452 Tokens per Sec: 20332.408909\n",
            "Epoch Step: 900 Loss: 11.997816 Tokens per Sec: 19949.165258\n",
            "Epoch Step: 1000 Loss: 41.033405 Tokens per Sec: 19449.814543\n",
            "Epoch Step: 1100 Loss: 35.368114 Tokens per Sec: 19369.603781\n",
            "Epoch Step: 1200 Loss: 38.562099 Tokens per Sec: 20076.997282\n",
            "Epoch Step: 1300 Loss: 19.532902 Tokens per Sec: 19535.972428\n",
            "Epoch Step: 1400 Loss: 44.608551 Tokens per Sec: 19894.751143\n",
            "Epoch Step: 1500 Loss: 40.782856 Tokens per Sec: 20190.343206\n",
            "Epoch Step: 1600 Loss: 6.531720 Tokens per Sec: 19818.922589\n",
            "Epoch Step: 1700 Loss: 21.393633 Tokens per Sec: 19747.705794\n",
            "Epoch Step: 1800 Loss: 22.437323 Tokens per Sec: 19794.795918\n",
            "Epoch Step: 1900 Loss: 25.972010 Tokens per Sec: 19823.490169\n",
            "Epoch Step: 2000 Loss: 26.848970 Tokens per Sec: 20156.675665\n",
            "Epoch Step: 2100 Loss: 58.268574 Tokens per Sec: 19556.458676\n",
            "Epoch Step: 2200 Loss: 53.836754 Tokens per Sec: 19343.635169\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was a guy from the morning of the <unk> of happiness .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father listened to his little <unk> , the radio shack of the bbc of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a very happy , which was pretty unusual , because the news was the <unk> .\n",
            "\n",
            "Validation perplexity: 11.672504\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 29.299234 Tokens per Sec: 18525.657786\n",
            "Epoch Step: 200 Loss: 13.587846 Tokens per Sec: 19688.689864\n",
            "Epoch Step: 300 Loss: 41.026218 Tokens per Sec: 19522.051483\n",
            "Epoch Step: 400 Loss: 19.172222 Tokens per Sec: 19413.051171\n",
            "Epoch Step: 500 Loss: 34.100243 Tokens per Sec: 19222.597914\n",
            "Epoch Step: 600 Loss: 46.441689 Tokens per Sec: 19380.598517\n",
            "Epoch Step: 700 Loss: 12.669528 Tokens per Sec: 19805.399207\n",
            "Epoch Step: 800 Loss: 39.332233 Tokens per Sec: 19908.009324\n",
            "Epoch Step: 900 Loss: 48.019775 Tokens per Sec: 19743.423806\n",
            "Epoch Step: 1000 Loss: 37.352177 Tokens per Sec: 19967.183501\n",
            "Epoch Step: 1100 Loss: 4.370196 Tokens per Sec: 19615.538389\n",
            "Epoch Step: 1200 Loss: 42.969498 Tokens per Sec: 19527.608027\n",
            "Epoch Step: 1300 Loss: 40.166870 Tokens per Sec: 19663.802786\n",
            "Epoch Step: 1400 Loss: 19.453484 Tokens per Sec: 19413.170474\n",
            "Epoch Step: 1500 Loss: 22.350584 Tokens per Sec: 20007.570443\n",
            "Epoch Step: 1600 Loss: 29.280151 Tokens per Sec: 20026.162643\n",
            "Epoch Step: 1700 Loss: 27.304480 Tokens per Sec: 19603.861695\n",
            "Epoch Step: 1800 Loss: 53.873837 Tokens per Sec: 19534.862972\n",
            "Epoch Step: 1900 Loss: 15.911556 Tokens per Sec: 19667.314363\n",
            "Epoch Step: 2000 Loss: 48.652039 Tokens per Sec: 19536.289915\n",
            "Epoch Step: 2100 Loss: 9.705561 Tokens per Sec: 19820.854735\n",
            "Epoch Step: 2200 Loss: 49.700943 Tokens per Sec: 19970.931129\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was going to wake up by the morning of the <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little <unk> , the radio shack of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a very happy , which was pretty unusual , because the news was <unk> .\n",
            "\n",
            "Validation perplexity: 11.712325\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 14.020564 Tokens per Sec: 19090.614407\n",
            "Epoch Step: 200 Loss: 8.966384 Tokens per Sec: 19488.289675\n",
            "Epoch Step: 300 Loss: 28.397654 Tokens per Sec: 20189.000973\n",
            "Epoch Step: 400 Loss: 21.600760 Tokens per Sec: 20093.911776\n",
            "Epoch Step: 500 Loss: 11.452582 Tokens per Sec: 19790.048885\n",
            "Epoch Step: 600 Loss: 42.843090 Tokens per Sec: 19987.661024\n",
            "Epoch Step: 700 Loss: 27.629726 Tokens per Sec: 19650.070681\n",
            "Epoch Step: 800 Loss: 32.321217 Tokens per Sec: 20084.966868\n",
            "Epoch Step: 900 Loss: 30.486429 Tokens per Sec: 19750.663020\n",
            "Epoch Step: 1000 Loss: 52.005722 Tokens per Sec: 19874.320426\n",
            "Epoch Step: 1100 Loss: 31.631004 Tokens per Sec: 20130.032480\n",
            "Epoch Step: 1200 Loss: 44.985435 Tokens per Sec: 19914.258299\n",
            "Epoch Step: 1300 Loss: 3.982891 Tokens per Sec: 19573.886106\n",
            "Epoch Step: 1400 Loss: 10.607178 Tokens per Sec: 19582.199456\n",
            "Epoch Step: 1500 Loss: 24.434645 Tokens per Sec: 19582.037682\n",
            "Epoch Step: 1600 Loss: 11.354562 Tokens per Sec: 19924.238658\n",
            "Epoch Step: 1700 Loss: 15.402143 Tokens per Sec: 20001.162753\n",
            "Epoch Step: 1800 Loss: 34.691761 Tokens per Sec: 19934.122427\n",
            "Epoch Step: 1900 Loss: 19.218487 Tokens per Sec: 19659.078037\n",
            "Epoch Step: 2000 Loss: 43.298000 Tokens per Sec: 20247.510684\n",
            "Epoch Step: 2100 Loss: 34.247894 Tokens per Sec: 20075.035395\n",
            "Epoch Step: 2200 Loss: 7.404073 Tokens per Sec: 19772.569791\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years , i was a <unk> of the <unk> <unk> .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was sitting on his little <unk> , the radio shack of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw very happy , which was pretty unusual , because the news was the <unk> .\n",
            "\n",
            "Validation perplexity: 11.804351\n",
            "Epoch 9\n",
            "Epoch Step: 100 Loss: 46.058918 Tokens per Sec: 19160.646435\n",
            "Epoch Step: 200 Loss: 29.375263 Tokens per Sec: 20139.086018\n",
            "Epoch Step: 300 Loss: 11.284271 Tokens per Sec: 19931.960005\n",
            "Epoch Step: 400 Loss: 40.545780 Tokens per Sec: 19733.357160\n",
            "Epoch Step: 500 Loss: 42.500706 Tokens per Sec: 19897.896065\n",
            "Epoch Step: 600 Loss: 2.269454 Tokens per Sec: 19817.889597\n",
            "Epoch Step: 700 Loss: 23.110437 Tokens per Sec: 19650.165126\n",
            "Epoch Step: 800 Loss: 50.563591 Tokens per Sec: 19839.860437\n",
            "Epoch Step: 900 Loss: 47.488991 Tokens per Sec: 19873.538059\n",
            "Epoch Step: 1000 Loss: 17.459887 Tokens per Sec: 19733.570297\n",
            "Epoch Step: 1100 Loss: 26.517073 Tokens per Sec: 20220.763936\n",
            "Epoch Step: 1200 Loss: 10.311320 Tokens per Sec: 19864.956381\n",
            "Epoch Step: 1300 Loss: 47.371002 Tokens per Sec: 19788.364371\n",
            "Epoch Step: 1400 Loss: 35.277077 Tokens per Sec: 19800.930430\n",
            "Epoch Step: 1500 Loss: 48.219868 Tokens per Sec: 19813.490608\n",
            "Epoch Step: 1600 Loss: 28.747972 Tokens per Sec: 20210.950642\n",
            "Epoch Step: 1700 Loss: 43.618969 Tokens per Sec: 20270.937537\n",
            "Epoch Step: 1800 Loss: 17.248249 Tokens per Sec: 20189.595841\n",
            "Epoch Step: 1900 Loss: 39.239258 Tokens per Sec: 20272.560156\n",
            "Epoch Step: 2000 Loss: 47.010574 Tokens per Sec: 19918.891011\n",
            "Epoch Step: 2100 Loss: 28.523563 Tokens per Sec: 19711.533061\n",
            "Epoch Step: 2200 Loss: 23.379507 Tokens per Sec: 20161.762862\n",
            "\n",
            "Example #1\n",
            "Src :  als ich 11 jahre alt war , wurde ich eines morgens von den <unk> heller freude geweckt .\n",
            "Trg :  when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
            "Pred:  when i was 11 years old , i was going to wake up the morning of the <unk> pink .\n",
            "\n",
            "Example #2\n",
            "Src :  mein vater hörte sich auf seinem kleinen , grauen radio die <unk> der bbc an .\n",
            "Trg :  my father was listening to bbc news on his small , gray radio .\n",
            "Pred:  my father was on his little <unk> , called the <unk> <unk> of the bbc .\n",
            "\n",
            "Example #3\n",
            "Src :  er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens <unk> .\n",
            "Trg :  there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
            "Pred:  he saw a very happy time , which was pretty much , because the messages were <unk> .\n",
            "\n",
            "Validation perplexity: 11.936926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56UGbb2tmUeN"
      },
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11H_-t1gapS"
      },
      "source": [
        "torch.save(model.state_dict(), 'annotated-encoder-decoder-de-en.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLFFDZldmtlp",
        "outputId": "569ba994-15c4-489f-b985-ab35eb1807e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsSt9Rb5V2fh"
      },
      "source": [
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtYodDoCV7UG",
        "outputId": "065d414d-4179-4a9e-9d5c-05e4de24a2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "state_dict = torch.load('annotated-encoder-decoder-de-en.pt')\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcC2kWxNzIs"
      },
      "source": [
        "## Save metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoWYohvQLjhJ"
      },
      "source": [
        "def save_meta(meta, path):\n",
        "    import pickle\n",
        "    output = open(path, 'wb')\n",
        "    pickle.dump(meta, output)\n",
        "    output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONr3RP7INJQG"
      },
      "source": [
        "def load_meta(path):\n",
        "    import pickle\n",
        "    inp = open(path, \"rb\")\n",
        "    meta = pickle.load(inp)\n",
        "    inp.close()\n",
        "    \n",
        "    return meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUlMmjRhL8Qs"
      },
      "source": [
        "save_meta({\n",
        "    \"UNK_TOKEN\": \"<unk>\",\n",
        "    \"PAD_TOKEN\": \"<pad>\",    \n",
        "    \"SOS_TOKEN\": \"<s>\",\n",
        "    \"EOS_TOKEN\": \"</s>\",\n",
        "    \"TRG.vocab.itos\": TRG.vocab.itos,\n",
        "    \"TRG.vocab.stoi\": TRG.vocab.stoi,\n",
        "    \"SRC.vocab.itos\": SRC.vocab.itos,\n",
        "    \"SRC.vocab.stoi\": SRC.vocab.stoi,\n",
        "}, 'de-to-en-meta.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC0RO53INidE"
      },
      "source": [
        "meta = load_meta('/content/de-to-en-meta.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVPM_OPHNl6j",
        "outputId": "952bc1bd-719b-4930-9160-5f4789f21a8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "meta.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['UNK_TOKEN', 'PAD_TOKEN', 'SOS_TOKEN', 'EOS_TOKEN', 'TRG.vocab.itos', 'TRG.vocab.stoi', 'SRC.vocab.itos', 'SRC.vocab.stoi'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    }
  ]
}