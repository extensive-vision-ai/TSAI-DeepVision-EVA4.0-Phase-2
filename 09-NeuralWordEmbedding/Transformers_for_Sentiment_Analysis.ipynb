{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Transformers for Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VnJJZZuz9Tng",
        "i9E722Z39Tn5",
        "8Fk_a0ap9ToY"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4060effff4d2411f8eb21a7df8bb06e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e2fd15b00c7422d95b0c7a090096d2c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21ec1eeeeeca43919357f61491cdaf33",
              "IPY_MODEL_10a21ff88ef54032a6bd23dee1d81a31"
            ]
          }
        },
        "4e2fd15b00c7422d95b0c7a090096d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21ec1eeeeeca43919357f61491cdaf33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c944a0a7ef2d4edda77b66fc05344d83",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce41035af80b4524835ea3ac149db1c6"
          }
        },
        "10a21ff88ef54032a6bd23dee1d81a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_272545778d0247979c6f97013173bc44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:09&lt;00:00, 43.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae023c06fcba4b0e95815540e1aaae15"
          }
        },
        "c944a0a7ef2d4edda77b66fc05344d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce41035af80b4524835ea3ac149db1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "272545778d0247979c6f97013173bc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae023c06fcba4b0e95815540e1aaae15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d70c956a1ad450caf2dc4d731ff17d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba53bbccb3be4fe98a891ecf1212a097",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4ac1897172b140bcb813ddd6da13666b",
              "IPY_MODEL_67d264c44a0d48aeb43c5c291f2f1764"
            ]
          }
        },
        "ba53bbccb3be4fe98a891ecf1212a097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ac1897172b140bcb813ddd6da13666b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5c4b8d0f68a747d5a2bcd7e22ce7ea6c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71d07d83bdd7448ea4d0afd3b13927af"
          }
        },
        "67d264c44a0d48aeb43c5c291f2f1764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_50f111557cf74f30a5e24c6cb217f528",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 58.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af8811e2ef724adbba0e4ebb7d73b30f"
          }
        },
        "5c4b8d0f68a747d5a2bcd7e22ce7ea6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71d07d83bdd7448ea4d0afd3b13927af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50f111557cf74f30a5e24c6cb217f528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af8811e2ef724adbba0e4ebb7d73b30f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepVision-EVA4.0-Phase-2/blob/master/09-NeuralWordEmbedding/Transformers_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZehsKeex9TmM"
      },
      "source": [
        "# 6 - Transformers for Sentiment Analysis\n",
        "\n",
        "In this notebook we will be using the transformer model, first introduced in [this](https://arxiv.org/abs/1706.03762) paper. Specifically, we will be using the BERT (Bidirectional Encoder Representations from Transformers) model from [this](https://arxiv.org/abs/1810.04805) paper. \n",
        "\n",
        "Transformer models are considerably larger than anything else covered in these tutorials. As such we are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as our embedding layers. We will freeze (not train) the transformer and only train the remainder of the model which learns from the representations produced by the transformer. In this case we will be using a multi-layer bi-directional GRU, however any model can learn from these representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DySe9rIvCN1C",
        "outputId": "75cbba33-cfcb-4e7e-9eab-7a602b98ea3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT6d9jqL9TmN"
      },
      "source": [
        "## Preparing Data\n",
        "\n",
        "First, as always, let's set the random seeds for deterministic results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFRRiUWo9TmO"
      },
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjXUlZYJ9TmS"
      },
      "source": [
        "The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n",
        "\n",
        "Luckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word). We get this by loading the pre-trained `bert-base-uncased` tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC9ROSPN9TmT"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAKKgYr39TmW"
      },
      "source": [
        "The `tokenizer` has a `vocab` attribute which contains the actual vocabulary we will be using. We can check how many tokens are in it by checking its length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_AkKW7-9TmX",
        "outputId": "bad5d798-3a24-45f2-cb09-24771ab1f970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fpEQjN49Tme"
      },
      "source": [
        "Using the tokenizer is as simple as calling `tokenizer.tokenize` on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUsvV84m9Tmf",
        "outputId": "b70a6c14-0a99-4d74-a3c6-d8eb0766bca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU? meow satyajit computer')\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'how', 'are', 'you', '?', 'me', '##ow', 'sat', '##ya', '##jit', 'computer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwwquVFD9Tmj"
      },
      "source": [
        "We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Odx6LaR9Tmk",
        "outputId": "2d9d7119-e6eb-49ee-820e-c30b57d32fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7592, 2088, 2129, 2024, 2017, 1029, 2033, 5004, 2938, 3148, 18902, 3274]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVweA3I99Tmr"
      },
      "source": [
        "The transformer was also trained with special tokens to mark the beginning and end of the sentence, detailed [here](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel). As well as a standard padding and unknown token. We can also get these from the tokenizer.\n",
        "\n",
        "**Note**: the tokenizer does have a beginning of sequence and end of sequence attributes (`bos_token` and `eos_token`) but these are not set and should not be used for this transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVEiAh719Tmr",
        "outputId": "0581a74e-85e4-4bb3-e39c-6b43bbf6f0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnVgKRMC9Tmy"
      },
      "source": [
        "We can get the indexes of the special tokens by converting them using the vocabulary..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6CU92g19Tmz",
        "outputId": "4b711b7a-ee9b-44dd-d2aa-d1042fb2a697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkAkQS909Tm3"
      },
      "source": [
        "...or by explicitly getting them from the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kniOQXrD9Tm4",
        "outputId": "281a184a-7ca1-4a08-c1a4-fe6d5ab5322d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM6jQA0c9Tm9"
      },
      "source": [
        "Another thing we need to handle is that the model was trained on sequences with a defined maximum length - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAsfqY_j9Tm-",
        "outputId": "9be0e421-2caf-4fa7-d2b7-930f44234c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKLvPZLm9TnB"
      },
      "source": [
        "Previously we have used the `spaCy` tokenizer to tokenize our examples. However we now need to define a function that we will pass to our `TEXT` field that will handle all the tokenization for us. It will also cut down the number of tokens to a maximum length. Note that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsHXIWB39TnB"
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzLeHxDP9TnE"
      },
      "source": [
        "Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n",
        "\n",
        "We define the label field as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5hs38pH9TnF"
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpFx8zxd9TnI"
      },
      "source": [
        "We load the data and create the validation splits as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ2wffmR9TnK"
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGzvNsYA9TnN",
        "outputId": "1a393cf0-dcc3-4575-c0da-dfc7a0acff61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcLTVNZD9TnQ"
      },
      "source": [
        "We can check an example and ensure that the text has already been numericalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1jhHT109TnR",
        "outputId": "717e9ebc-6e12-4b68-be7b-0ef00efd9a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(vars(train_data.examples[6]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': [5621, 2028, 1997, 1996, 2087, 18704, 3152, 1045, 1005, 2310, 2412, 2938, 2083, 1012, 1045, 1005, 2310, 2196, 2941, 2579, 1996, 2051, 2000, 4339, 2028, 1997, 2122, 2021, 2371, 15055, 2000, 2044, 24740, 2023, 21358, 12792, 2000, 2143, 1011, 2437, 1998, 2514, 5399, 12943, 16523, 2666, 7178, 2000, 2022, 18313, 2026, 2051, 2006, 2107, 1037, 3538, 1997, 10722, 4103, 2000, 2022, 7481, 1012, 2045, 2020, 2061, 2116, 3033, 2008, 1999, 27942, 15070, 2033, 2007, 2037, 3143, 6721, 2791, 1998, 3768, 1997, 3168, 1006, 1041, 1012, 1043, 1012, 2043, 2052, 1996, 2610, 2486, 2412, 5607, 2111, 2007, 16514, 7870, 1029, 2043, 2052, 8323, 2412, 2083, 2041, 2107, 2111, 2005, 3768, 1997, 1037, 9526, 1029, 2339, 2001, 1996, 3124, 2040, 7282, 2032, 22624, 2006, 2010, 2564, 13071, 2105, 2648, 1999, 2010, 11225, 11739, 5819, 4755, 1037, 3282, 2004, 2016, 4565, 2105, 2006, 1996, 2793, 1029, 1007, 1012, 2036, 1010, 1996, 2839, 6648, 1011, 2004, 2057, 1005, 2310, 2471, 2272, 2000, 5987, 1999, 2107, 3152, 1011, 2001, 9643, 1006, 1041, 1012, 1043, 1012, 1996, 2126, 1996, 9081, 3124, 1011, 1045, 2123, 1005, 1056, 3342, 2010, 10424, 6799, 2378, 2171, 1998, 2123, 1005, 1056, 2507, 1037, 10055, 4312, 1011, 3294, 2357, 2114, 2010, 6513, 1998, 2743, 2125, 2000, 2681, 2014, 1007, 1998, 1045, 3092, 2039, 5782, 2068, 2035, 2000, 3113, 24665, 2483, 2135, 4515, 999, 1996, 2537, 2001, 27762, 4487, 2015, 5558, 18447, 2098, 1998, 1996, 16434, 2498, 2000, 4339, 2188, 2055, 1012], 'label': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4K9hbwY9TnU"
      },
      "source": [
        "We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHe_mZke9TnV",
        "outputId": "4ebad7c7-592f-41df-862c-6a3c2423b714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['truly', 'one', 'of', 'the', 'most', 'dire', 'films', 'i', \"'\", 've', 'ever', 'sat', 'through', '.', 'i', \"'\", 've', 'never', 'actually', 'taken', 'the', 'time', 'to', 'write', 'one', 'of', 'these', 'but', 'felt', 'compelled', 'to', 'after', 'witnessing', 'this', 'af', '##front', 'to', 'film', '-', 'making', 'and', 'feel', 'somewhat', 'ag', '##gr', '##ie', '##ved', 'to', 'be', 'wasting', 'my', 'time', 'on', 'such', 'a', 'piece', 'of', 'tu', '##rd', 'to', 'be', 'honest', '.', 'there', 'were', 'so', 'many', 'parts', 'that', 'in', '##fur', '##iated', 'me', 'with', 'their', 'complete', 'random', '##ness', 'and', 'lack', 'of', 'sense', '(', 'e', '.', 'g', '.', 'when', 'would', 'the', 'police', 'force', 'ever', 'shoot', 'people', 'with', 'infectious', 'diseases', '?', 'when', 'would', 'hospitals', 'ever', 'through', 'out', 'such', 'people', 'for', 'lack', 'of', 'a', 'cure', '?', 'why', 'was', 'the', 'guy', 'who', 'spotted', 'him', 'spying', 'on', 'his', 'wife', 'wandering', 'around', 'outside', 'in', 'his', 'dressing', 'gown', 'whilst', 'carrying', 'a', 'gun', 'as', 'she', 'rolled', 'around', 'on', 'the', 'bed', '?', ')', '.', 'also', ',', 'the', 'character', '##isation', '-', 'as', 'we', \"'\", 've', 'almost', 'come', 'to', 'expect', 'in', 'such', 'films', '-', 'was', 'awful', '(', 'e', '.', 'g', '.', 'the', 'way', 'the', 'blonde', 'guy', '-', 'i', 'don', \"'\", 't', 'remember', 'his', 'fr', '##ick', '##in', 'name', 'and', 'don', \"'\", 't', 'give', 'a', 'toss', 'anyway', '-', 'completely', 'turned', 'against', 'his', 'girlfriend', 'and', 'ran', 'off', 'to', 'leave', 'her', ')', 'and', 'i', 'ended', 'up', 'wanting', 'them', 'all', 'to', 'meet', 'gr', '##is', '##ly', 'ends', '!', 'the', 'production', 'was', 'horribly', 'di', '##s', '##jo', '##int', '##ed', 'and', 'the', 'cinematography', 'nothing', 'to', 'write', 'home', 'about', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JTK9Dfv9TnX"
      },
      "source": [
        "Although we've handled the vocabulary for the text, we still need to build the vocabulary for the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK_6t8ws9TnY"
      },
      "source": [
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaTN43lc9Tnb",
        "outputId": "3ccea25f-6f02-47de-bf58-00832663e9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f894f3cd730>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKSFStqL9Tnd"
      },
      "source": [
        "As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVZIlpII9Tne"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq7VMLJyPArp"
      },
      "source": [
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um8WF0GzPGmY",
        "outputId": "504329e1-2931-4fbc-a9c9-d5f2fae4a5dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 128]\n",
              "\t[.text]:[torch.LongTensor of size 128x512]\n",
              "\t[.label]:[torch.FloatTensor of size 128]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnJJZZuz9Tng"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Next, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gR1XmRn9Tng",
        "outputId": "d9e40409-36d1-47e8-c5d9-8bee2fe571b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "4060effff4d2411f8eb21a7df8bb06e3",
            "4e2fd15b00c7422d95b0c7a090096d2c",
            "21ec1eeeeeca43919357f61491cdaf33",
            "10a21ff88ef54032a6bd23dee1d81a31",
            "c944a0a7ef2d4edda77b66fc05344d83",
            "ce41035af80b4524835ea3ac149db1c6",
            "272545778d0247979c6f97013173bc44",
            "ae023c06fcba4b0e95815540e1aaae15",
            "0d70c956a1ad450caf2dc4d731ff17d0",
            "ba53bbccb3be4fe98a891ecf1212a097",
            "4ac1897172b140bcb813ddd6da13666b",
            "67d264c44a0d48aeb43c5c291f2f1764",
            "5c4b8d0f68a747d5a2bcd7e22ce7ea6c",
            "71d07d83bdd7448ea4d0afd3b13927af",
            "50f111557cf74f30a5e24c6cb217f528",
            "af8811e2ef724adbba0e4ebb7d73b30f"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4060effff4d2411f8eb21a7df8bb06e3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d70c956a1ad450caf2dc4d731ff17d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3joG4t_i9Tnj"
      },
      "source": [
        "Next, we'll define our actual model. \n",
        "\n",
        "Instead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the sentiment of the input sentence. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n",
        "\n",
        "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, youâ€™re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q59Q-q1k9Tnk"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J2Xdm329Tnm"
      },
      "source": [
        "Next, we create an instance of our model using standard hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCqEPHwV9Tnm"
      },
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGfjil9H9Tnq"
      },
      "source": [
        "We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcR8dWIJ9Tnr",
        "outputId": "fd477a7e-b288-469e-e18d-95e11126c5a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 112,241,409 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6DPt2lz9Tnu"
      },
      "source": [
        "In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H5m4mFq9Tnx"
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kxa_DD_9Tn0"
      },
      "source": [
        "We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFXqM-9K9Tn0",
        "outputId": "e6005635-fcbe-4c4a-f407-22e62187e7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,759,169 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZwB9A0y9Tn2"
      },
      "source": [
        "We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (`rnn`) and the linear layer (`out`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sILMOwg9Tn3",
        "outputId": "8b4e33d5-a29a-4e4e-a5fb-5ed6b79c4d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWeYviaVPd3Q",
        "outputId": "a5ce50de-1999-4373-99e1-2e5e7972dfe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "! pip install torchsummaryX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading https://files.pythonhosted.org/packages/36/23/87eeaaf70daa61aa21495ece0969c50c446b8fd42c4b8905af264b40fe7f/torchsummaryX-1.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchsummaryX) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V98awLblO2xs"
      },
      "source": [
        "from torchsummaryX import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjy9giulPPBR",
        "outputId": "a65d03f6-bb8f-48a0-e97d-260bba59a080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inputs = torch.zeros((512, 1), dtype=torch.long) # [length, batch_size]\n",
        "summary(model, inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===================================================================================================================\n",
            "                                                    Kernel Shape  \\\n",
            "Layer                                                              \n",
            "0_bert.embeddings.Embedding_word_embeddings         [768, 30522]   \n",
            "1_bert.embeddings.Embedding_position_embeddings       [768, 512]   \n",
            "2_bert.embeddings.Embedding_token_type_embeddings       [768, 2]   \n",
            "3_bert.embeddings.LayerNorm_LayerNorm                      [768]   \n",
            "4_bert.embeddings.Dropout_dropout                              -   \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query    [768, 768]   \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key      [768, 768]   \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value    [768, 768]   \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...             -   \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...    [768, 768]   \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...             -   \n",
            "11_bert.encoder.layer.0.attention.output.LayerN...         [768]   \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense    [768, 3072]   \n",
            "13_bert.encoder.layer.0.output.Linear_dense          [3072, 768]   \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout                 -   \n",
            "15_bert.encoder.layer.0.output.LayerNorm_LayerNorm         [768]   \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...    [768, 768]   \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key     [768, 768]   \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...    [768, 768]   \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...             -   \n",
            "20_bert.encoder.layer.1.attention.output.Linear...    [768, 768]   \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...             -   \n",
            "22_bert.encoder.layer.1.attention.output.LayerN...         [768]   \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense    [768, 3072]   \n",
            "24_bert.encoder.layer.1.output.Linear_dense          [3072, 768]   \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout                 -   \n",
            "26_bert.encoder.layer.1.output.LayerNorm_LayerNorm         [768]   \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...    [768, 768]   \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key     [768, 768]   \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...    [768, 768]   \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...             -   \n",
            "31_bert.encoder.layer.2.attention.output.Linear...    [768, 768]   \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...             -   \n",
            "33_bert.encoder.layer.2.attention.output.LayerN...         [768]   \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense    [768, 3072]   \n",
            "35_bert.encoder.layer.2.output.Linear_dense          [3072, 768]   \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout                 -   \n",
            "37_bert.encoder.layer.2.output.LayerNorm_LayerNorm         [768]   \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...    [768, 768]   \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key     [768, 768]   \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...    [768, 768]   \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...             -   \n",
            "42_bert.encoder.layer.3.attention.output.Linear...    [768, 768]   \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...             -   \n",
            "44_bert.encoder.layer.3.attention.output.LayerN...         [768]   \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense    [768, 3072]   \n",
            "46_bert.encoder.layer.3.output.Linear_dense          [3072, 768]   \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout                 -   \n",
            "48_bert.encoder.layer.3.output.LayerNorm_LayerNorm         [768]   \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...    [768, 768]   \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key     [768, 768]   \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...    [768, 768]   \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...             -   \n",
            "53_bert.encoder.layer.4.attention.output.Linear...    [768, 768]   \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...             -   \n",
            "55_bert.encoder.layer.4.attention.output.LayerN...         [768]   \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense    [768, 3072]   \n",
            "57_bert.encoder.layer.4.output.Linear_dense          [3072, 768]   \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout                 -   \n",
            "59_bert.encoder.layer.4.output.LayerNorm_LayerNorm         [768]   \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...    [768, 768]   \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key     [768, 768]   \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...    [768, 768]   \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...             -   \n",
            "64_bert.encoder.layer.5.attention.output.Linear...    [768, 768]   \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...             -   \n",
            "66_bert.encoder.layer.5.attention.output.LayerN...         [768]   \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense    [768, 3072]   \n",
            "68_bert.encoder.layer.5.output.Linear_dense          [3072, 768]   \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout                 -   \n",
            "70_bert.encoder.layer.5.output.LayerNorm_LayerNorm         [768]   \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...    [768, 768]   \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key     [768, 768]   \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...    [768, 768]   \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...             -   \n",
            "75_bert.encoder.layer.6.attention.output.Linear...    [768, 768]   \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...             -   \n",
            "77_bert.encoder.layer.6.attention.output.LayerN...         [768]   \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense    [768, 3072]   \n",
            "79_bert.encoder.layer.6.output.Linear_dense          [3072, 768]   \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout                 -   \n",
            "81_bert.encoder.layer.6.output.LayerNorm_LayerNorm         [768]   \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...    [768, 768]   \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key     [768, 768]   \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...    [768, 768]   \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...             -   \n",
            "86_bert.encoder.layer.7.attention.output.Linear...    [768, 768]   \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...             -   \n",
            "88_bert.encoder.layer.7.attention.output.LayerN...         [768]   \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense    [768, 3072]   \n",
            "90_bert.encoder.layer.7.output.Linear_dense          [3072, 768]   \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout                 -   \n",
            "92_bert.encoder.layer.7.output.LayerNorm_LayerNorm         [768]   \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...    [768, 768]   \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key     [768, 768]   \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...    [768, 768]   \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...             -   \n",
            "97_bert.encoder.layer.8.attention.output.Linear...    [768, 768]   \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...             -   \n",
            "99_bert.encoder.layer.8.attention.output.LayerN...         [768]   \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "101_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "103_bert.encoder.layer.8.output.LayerNorm_Layer...         [768]   \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "108_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "110_bert.encoder.layer.9.attention.output.Layer...         [768]   \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "112_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "114_bert.encoder.layer.9.output.LayerNorm_Layer...         [768]   \n",
            "115_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "116_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "117_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "119_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "120_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "121_bert.encoder.layer.10.attention.output.Laye...         [768]   \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "123_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "125_bert.encoder.layer.10.output.LayerNorm_Laye...         [768]   \n",
            "126_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "127_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "128_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "130_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "131_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "132_bert.encoder.layer.11.attention.output.Laye...         [768]   \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "134_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "136_bert.encoder.layer.11.output.LayerNorm_Laye...         [768]   \n",
            "137_bert.pooler.Linear_dense                          [768, 768]   \n",
            "138_bert.pooler.Tanh_activation                                -   \n",
            "139_rnn                                                        -   \n",
            "140_dropout                                                    -   \n",
            "141_out                                                 [512, 1]   \n",
            "\n",
            "                                                       Output Shape  \\\n",
            "Layer                                                                 \n",
            "0_bert.embeddings.Embedding_word_embeddings           [512, 1, 768]   \n",
            "1_bert.embeddings.Embedding_position_embeddings         [1, 1, 768]   \n",
            "2_bert.embeddings.Embedding_token_type_embeddings     [512, 1, 768]   \n",
            "3_bert.embeddings.LayerNorm_LayerNorm                 [512, 1, 768]   \n",
            "4_bert.embeddings.Dropout_dropout                     [512, 1, 768]   \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query    [512, 1, 768]   \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key      [512, 1, 768]   \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value    [512, 1, 768]   \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...  [512, 12, 1, 1]   \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...    [512, 1, 768]   \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...    [512, 1, 768]   \n",
            "11_bert.encoder.layer.0.attention.output.LayerN...    [512, 1, 768]   \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "13_bert.encoder.layer.0.output.Linear_dense           [512, 1, 768]   \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout        [512, 1, 768]   \n",
            "15_bert.encoder.layer.0.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...    [512, 1, 768]   \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key     [512, 1, 768]   \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...    [512, 1, 768]   \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "20_bert.encoder.layer.1.attention.output.Linear...    [512, 1, 768]   \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...    [512, 1, 768]   \n",
            "22_bert.encoder.layer.1.attention.output.LayerN...    [512, 1, 768]   \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "24_bert.encoder.layer.1.output.Linear_dense           [512, 1, 768]   \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout        [512, 1, 768]   \n",
            "26_bert.encoder.layer.1.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...    [512, 1, 768]   \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key     [512, 1, 768]   \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...    [512, 1, 768]   \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "31_bert.encoder.layer.2.attention.output.Linear...    [512, 1, 768]   \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...    [512, 1, 768]   \n",
            "33_bert.encoder.layer.2.attention.output.LayerN...    [512, 1, 768]   \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "35_bert.encoder.layer.2.output.Linear_dense           [512, 1, 768]   \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout        [512, 1, 768]   \n",
            "37_bert.encoder.layer.2.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...    [512, 1, 768]   \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key     [512, 1, 768]   \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...    [512, 1, 768]   \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "42_bert.encoder.layer.3.attention.output.Linear...    [512, 1, 768]   \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...    [512, 1, 768]   \n",
            "44_bert.encoder.layer.3.attention.output.LayerN...    [512, 1, 768]   \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "46_bert.encoder.layer.3.output.Linear_dense           [512, 1, 768]   \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout        [512, 1, 768]   \n",
            "48_bert.encoder.layer.3.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...    [512, 1, 768]   \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key     [512, 1, 768]   \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...    [512, 1, 768]   \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "53_bert.encoder.layer.4.attention.output.Linear...    [512, 1, 768]   \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...    [512, 1, 768]   \n",
            "55_bert.encoder.layer.4.attention.output.LayerN...    [512, 1, 768]   \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "57_bert.encoder.layer.4.output.Linear_dense           [512, 1, 768]   \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout        [512, 1, 768]   \n",
            "59_bert.encoder.layer.4.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...    [512, 1, 768]   \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key     [512, 1, 768]   \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...    [512, 1, 768]   \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "64_bert.encoder.layer.5.attention.output.Linear...    [512, 1, 768]   \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...    [512, 1, 768]   \n",
            "66_bert.encoder.layer.5.attention.output.LayerN...    [512, 1, 768]   \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "68_bert.encoder.layer.5.output.Linear_dense           [512, 1, 768]   \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout        [512, 1, 768]   \n",
            "70_bert.encoder.layer.5.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...    [512, 1, 768]   \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key     [512, 1, 768]   \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...    [512, 1, 768]   \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "75_bert.encoder.layer.6.attention.output.Linear...    [512, 1, 768]   \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...    [512, 1, 768]   \n",
            "77_bert.encoder.layer.6.attention.output.LayerN...    [512, 1, 768]   \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "79_bert.encoder.layer.6.output.Linear_dense           [512, 1, 768]   \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout        [512, 1, 768]   \n",
            "81_bert.encoder.layer.6.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...    [512, 1, 768]   \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key     [512, 1, 768]   \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...    [512, 1, 768]   \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "86_bert.encoder.layer.7.attention.output.Linear...    [512, 1, 768]   \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...    [512, 1, 768]   \n",
            "88_bert.encoder.layer.7.attention.output.LayerN...    [512, 1, 768]   \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense    [512, 1, 3072]   \n",
            "90_bert.encoder.layer.7.output.Linear_dense           [512, 1, 768]   \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout        [512, 1, 768]   \n",
            "92_bert.encoder.layer.7.output.LayerNorm_LayerNorm    [512, 1, 768]   \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...    [512, 1, 768]   \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key     [512, 1, 768]   \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...    [512, 1, 768]   \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...  [512, 12, 1, 1]   \n",
            "97_bert.encoder.layer.8.attention.output.Linear...    [512, 1, 768]   \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...    [512, 1, 768]   \n",
            "99_bert.encoder.layer.8.attention.output.LayerN...    [512, 1, 768]   \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense   [512, 1, 3072]   \n",
            "101_bert.encoder.layer.8.output.Linear_dense          [512, 1, 768]   \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout       [512, 1, 768]   \n",
            "103_bert.encoder.layer.8.output.LayerNorm_Layer...    [512, 1, 768]   \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...    [512, 1, 768]   \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key    [512, 1, 768]   \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...    [512, 1, 768]   \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...  [512, 12, 1, 1]   \n",
            "108_bert.encoder.layer.9.attention.output.Linea...    [512, 1, 768]   \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...    [512, 1, 768]   \n",
            "110_bert.encoder.layer.9.attention.output.Layer...    [512, 1, 768]   \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense   [512, 1, 3072]   \n",
            "112_bert.encoder.layer.9.output.Linear_dense          [512, 1, 768]   \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout       [512, 1, 768]   \n",
            "114_bert.encoder.layer.9.output.LayerNorm_Layer...    [512, 1, 768]   \n",
            "115_bert.encoder.layer.10.attention.self.Linear...    [512, 1, 768]   \n",
            "116_bert.encoder.layer.10.attention.self.Linear...    [512, 1, 768]   \n",
            "117_bert.encoder.layer.10.attention.self.Linear...    [512, 1, 768]   \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...  [512, 12, 1, 1]   \n",
            "119_bert.encoder.layer.10.attention.output.Line...    [512, 1, 768]   \n",
            "120_bert.encoder.layer.10.attention.output.Drop...    [512, 1, 768]   \n",
            "121_bert.encoder.layer.10.attention.output.Laye...    [512, 1, 768]   \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...   [512, 1, 3072]   \n",
            "123_bert.encoder.layer.10.output.Linear_dense         [512, 1, 768]   \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout      [512, 1, 768]   \n",
            "125_bert.encoder.layer.10.output.LayerNorm_Laye...    [512, 1, 768]   \n",
            "126_bert.encoder.layer.11.attention.self.Linear...    [512, 1, 768]   \n",
            "127_bert.encoder.layer.11.attention.self.Linear...    [512, 1, 768]   \n",
            "128_bert.encoder.layer.11.attention.self.Linear...    [512, 1, 768]   \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...  [512, 12, 1, 1]   \n",
            "130_bert.encoder.layer.11.attention.output.Line...    [512, 1, 768]   \n",
            "131_bert.encoder.layer.11.attention.output.Drop...    [512, 1, 768]   \n",
            "132_bert.encoder.layer.11.attention.output.Laye...    [512, 1, 768]   \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...   [512, 1, 3072]   \n",
            "134_bert.encoder.layer.11.output.Linear_dense         [512, 1, 768]   \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout      [512, 1, 768]   \n",
            "136_bert.encoder.layer.11.output.LayerNorm_Laye...    [512, 1, 768]   \n",
            "137_bert.pooler.Linear_dense                             [512, 768]   \n",
            "138_bert.pooler.Tanh_activation                          [512, 768]   \n",
            "139_rnn                                               [512, 1, 512]   \n",
            "140_dropout                                              [512, 512]   \n",
            "141_out                                                    [512, 1]   \n",
            "\n",
            "                                                       Params  Mult-Adds  \n",
            "Layer                                                                     \n",
            "0_bert.embeddings.Embedding_word_embeddings                 -          -  \n",
            "1_bert.embeddings.Embedding_position_embeddings             -          -  \n",
            "2_bert.embeddings.Embedding_token_type_embeddings           -          -  \n",
            "3_bert.embeddings.LayerNorm_LayerNorm                       -          -  \n",
            "4_bert.embeddings.Dropout_dropout                           -          -  \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query          -          -  \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key            -          -  \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value          -          -  \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...          -          -  \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...          -          -  \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...          -          -  \n",
            "11_bert.encoder.layer.0.attention.output.LayerN...          -          -  \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense           -          -  \n",
            "13_bert.encoder.layer.0.output.Linear_dense                 -          -  \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout              -          -  \n",
            "15_bert.encoder.layer.0.output.LayerNorm_LayerNorm          -          -  \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...          -          -  \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key           -          -  \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...          -          -  \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...          -          -  \n",
            "20_bert.encoder.layer.1.attention.output.Linear...          -          -  \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...          -          -  \n",
            "22_bert.encoder.layer.1.attention.output.LayerN...          -          -  \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense           -          -  \n",
            "24_bert.encoder.layer.1.output.Linear_dense                 -          -  \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout              -          -  \n",
            "26_bert.encoder.layer.1.output.LayerNorm_LayerNorm          -          -  \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...          -          -  \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key           -          -  \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...          -          -  \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...          -          -  \n",
            "31_bert.encoder.layer.2.attention.output.Linear...          -          -  \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...          -          -  \n",
            "33_bert.encoder.layer.2.attention.output.LayerN...          -          -  \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense           -          -  \n",
            "35_bert.encoder.layer.2.output.Linear_dense                 -          -  \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout              -          -  \n",
            "37_bert.encoder.layer.2.output.LayerNorm_LayerNorm          -          -  \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...          -          -  \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key           -          -  \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...          -          -  \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...          -          -  \n",
            "42_bert.encoder.layer.3.attention.output.Linear...          -          -  \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...          -          -  \n",
            "44_bert.encoder.layer.3.attention.output.LayerN...          -          -  \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense           -          -  \n",
            "46_bert.encoder.layer.3.output.Linear_dense                 -          -  \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout              -          -  \n",
            "48_bert.encoder.layer.3.output.LayerNorm_LayerNorm          -          -  \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...          -          -  \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key           -          -  \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...          -          -  \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...          -          -  \n",
            "53_bert.encoder.layer.4.attention.output.Linear...          -          -  \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...          -          -  \n",
            "55_bert.encoder.layer.4.attention.output.LayerN...          -          -  \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense           -          -  \n",
            "57_bert.encoder.layer.4.output.Linear_dense                 -          -  \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout              -          -  \n",
            "59_bert.encoder.layer.4.output.LayerNorm_LayerNorm          -          -  \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...          -          -  \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key           -          -  \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...          -          -  \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...          -          -  \n",
            "64_bert.encoder.layer.5.attention.output.Linear...          -          -  \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...          -          -  \n",
            "66_bert.encoder.layer.5.attention.output.LayerN...          -          -  \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense           -          -  \n",
            "68_bert.encoder.layer.5.output.Linear_dense                 -          -  \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout              -          -  \n",
            "70_bert.encoder.layer.5.output.LayerNorm_LayerNorm          -          -  \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...          -          -  \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key           -          -  \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...          -          -  \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...          -          -  \n",
            "75_bert.encoder.layer.6.attention.output.Linear...          -          -  \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...          -          -  \n",
            "77_bert.encoder.layer.6.attention.output.LayerN...          -          -  \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense           -          -  \n",
            "79_bert.encoder.layer.6.output.Linear_dense                 -          -  \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout              -          -  \n",
            "81_bert.encoder.layer.6.output.LayerNorm_LayerNorm          -          -  \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...          -          -  \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key           -          -  \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...          -          -  \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...          -          -  \n",
            "86_bert.encoder.layer.7.attention.output.Linear...          -          -  \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...          -          -  \n",
            "88_bert.encoder.layer.7.attention.output.LayerN...          -          -  \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense           -          -  \n",
            "90_bert.encoder.layer.7.output.Linear_dense                 -          -  \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout              -          -  \n",
            "92_bert.encoder.layer.7.output.LayerNorm_LayerNorm          -          -  \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...          -          -  \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key           -          -  \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...          -          -  \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...          -          -  \n",
            "97_bert.encoder.layer.8.attention.output.Linear...          -          -  \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...          -          -  \n",
            "99_bert.encoder.layer.8.attention.output.LayerN...          -          -  \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense          -          -  \n",
            "101_bert.encoder.layer.8.output.Linear_dense                -          -  \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout             -          -  \n",
            "103_bert.encoder.layer.8.output.LayerNorm_Layer...          -          -  \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...          -          -  \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key          -          -  \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...          -          -  \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...          -          -  \n",
            "108_bert.encoder.layer.9.attention.output.Linea...          -          -  \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...          -          -  \n",
            "110_bert.encoder.layer.9.attention.output.Layer...          -          -  \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense          -          -  \n",
            "112_bert.encoder.layer.9.output.Linear_dense                -          -  \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout             -          -  \n",
            "114_bert.encoder.layer.9.output.LayerNorm_Layer...          -          -  \n",
            "115_bert.encoder.layer.10.attention.self.Linear...          -          -  \n",
            "116_bert.encoder.layer.10.attention.self.Linear...          -          -  \n",
            "117_bert.encoder.layer.10.attention.self.Linear...          -          -  \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...          -          -  \n",
            "119_bert.encoder.layer.10.attention.output.Line...          -          -  \n",
            "120_bert.encoder.layer.10.attention.output.Drop...          -          -  \n",
            "121_bert.encoder.layer.10.attention.output.Laye...          -          -  \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...          -          -  \n",
            "123_bert.encoder.layer.10.output.Linear_dense               -          -  \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout            -          -  \n",
            "125_bert.encoder.layer.10.output.LayerNorm_Laye...          -          -  \n",
            "126_bert.encoder.layer.11.attention.self.Linear...          -          -  \n",
            "127_bert.encoder.layer.11.attention.self.Linear...          -          -  \n",
            "128_bert.encoder.layer.11.attention.self.Linear...          -          -  \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...          -          -  \n",
            "130_bert.encoder.layer.11.attention.output.Line...          -          -  \n",
            "131_bert.encoder.layer.11.attention.output.Drop...          -          -  \n",
            "132_bert.encoder.layer.11.attention.output.Laye...          -          -  \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...          -          -  \n",
            "134_bert.encoder.layer.11.output.Linear_dense               -          -  \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout            -          -  \n",
            "136_bert.encoder.layer.11.output.LayerNorm_Laye...          -          -  \n",
            "137_bert.pooler.Linear_dense                                -          -  \n",
            "138_bert.pooler.Tanh_activation                             -          -  \n",
            "139_rnn                                             2.758656M  2.752512M  \n",
            "140_dropout                                                 -          -  \n",
            "141_out                                                 513.0      512.0  \n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params          112.241409M\n",
            "Trainable params        2.759169M\n",
            "Non-trainable params   109.48224M\n",
            "Mult-Adds               2.753024M\n",
            "===================================================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_bert.embeddings.Embedding_word_embeddings</th>\n",
              "      <td>[768, 30522]</td>\n",
              "      <td>[512, 1, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_bert.embeddings.Embedding_position_embeddings</th>\n",
              "      <td>[768, 512]</td>\n",
              "      <td>[1, 1, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_bert.embeddings.Embedding_token_type_embeddings</th>\n",
              "      <td>[768, 2]</td>\n",
              "      <td>[512, 1, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_bert.embeddings.LayerNorm_LayerNorm</th>\n",
              "      <td>[768]</td>\n",
              "      <td>[512, 1, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_bert.embeddings.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 1, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137_bert.pooler.Linear_dense</th>\n",
              "      <td>[768, 768]</td>\n",
              "      <td>[512, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138_bert.pooler.Tanh_activation</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139_rnn</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 1, 512]</td>\n",
              "      <td>2758656.0</td>\n",
              "      <td>2752512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141_out</th>\n",
              "      <td>[512, 1]</td>\n",
              "      <td>[512, 1]</td>\n",
              "      <td>513.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Kernel Shape  ...  Mult-Adds\n",
              "Layer                                                            ...           \n",
              "0_bert.embeddings.Embedding_word_embeddings        [768, 30522]  ...        NaN\n",
              "1_bert.embeddings.Embedding_position_embeddings      [768, 512]  ...        NaN\n",
              "2_bert.embeddings.Embedding_token_type_embeddings      [768, 2]  ...        NaN\n",
              "3_bert.embeddings.LayerNorm_LayerNorm                     [768]  ...        NaN\n",
              "4_bert.embeddings.Dropout_dropout                             -  ...        NaN\n",
              "...                                                         ...  ...        ...\n",
              "137_bert.pooler.Linear_dense                         [768, 768]  ...        NaN\n",
              "138_bert.pooler.Tanh_activation                               -  ...        NaN\n",
              "139_rnn                                                       -  ...  2752512.0\n",
              "140_dropout                                                   -  ...        NaN\n",
              "141_out                                                [512, 1]  ...      512.0\n",
              "\n",
              "[142 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaL_dpximpTy"
      },
      "source": [
        "torch.save(model, 'model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9E722Z39Tn5"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "As is standard, we define our optimizer and criterion (loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NfNdLd79Tn5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLNV9bJR9Tn9"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QgsUkFa9ToA"
      },
      "source": [
        "Place the model and criterion onto the GPU (if available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J47eMw1n9ToA"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_8_E1Wi9ToC"
      },
      "source": [
        "Next, we'll define functions for: calculating accuracy, performing a training epoch, performing an evaluation epoch and calculating how long a training/evaluation epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3dVt3399ToC"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OOsja-59ToG"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjFFOfro9ToJ"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th7Bf9c89ToN"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvQC2ldI9ToR"
      },
      "source": [
        "Finally, we'll train our model. This takes considerably longer than any of the previous models due to the size of the transformer. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BcU__h39ToR",
        "outputId": "0a4601cd-9b32-43b9-bd5f-4ba17174ded6"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 7m 27s\n",
            "\tTrain Loss: 0.286 | Train Acc: 88.16%\n",
            "\t Val. Loss: 0.247 |  Val. Acc: 90.26%\n",
            "Epoch: 02 | Epoch Time: 7m 27s\n",
            "\tTrain Loss: 0.234 | Train Acc: 90.77%\n",
            "\t Val. Loss: 0.229 |  Val. Acc: 91.00%\n",
            "Epoch: 03 | Epoch Time: 7m 27s\n",
            "\tTrain Loss: 0.209 | Train Acc: 91.83%\n",
            "\t Val. Loss: 0.225 |  Val. Acc: 91.10%\n",
            "Epoch: 04 | Epoch Time: 7m 27s\n",
            "\tTrain Loss: 0.182 | Train Acc: 92.97%\n",
            "\t Val. Loss: 0.217 |  Val. Acc: 91.98%\n",
            "Epoch: 05 | Epoch Time: 7m 27s\n",
            "\tTrain Loss: 0.156 | Train Acc: 94.17%\n",
            "\t Val. Loss: 0.230 |  Val. Acc: 91.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpnHiXa89ToU"
      },
      "source": [
        "We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebFss5sr9ToU",
        "outputId": "09e44817-fc83-4981-cd68-1e3b17a6ec1c"
      },
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.198 | Test Acc: 92.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fk_a0ap9ToY"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbrTXQn69ToY"
      },
      "source": [
        "def predict_sentiment(model, tokenizer, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qRQE4D49Tof",
        "outputId": "94d107c5-27e2-44c1-8b9e-2986363b7b7a"
      },
      "source": [
        "predict_sentiment(model, tokenizer, \"This film is terrible\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02264496125280857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M_o_XjW9Tok",
        "outputId": "7cee769d-c423-4ed0-fc8b-7ce82270ba4f"
      },
      "source": [
        "predict_sentiment(model, tokenizer, \"This film is great\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9411056041717529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}