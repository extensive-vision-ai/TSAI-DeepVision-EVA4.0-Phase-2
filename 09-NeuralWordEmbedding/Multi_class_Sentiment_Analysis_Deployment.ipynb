{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Multi-class Sentiment Analysis-Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepVision-EVA4.0-Phase-2/blob/master/09-NeuralWordEmbedding/Multi_class_Sentiment_Analysis_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7iMBhs5oryl"
      },
      "source": [
        "# 5 - Multi-class Sentiment Analysis\n",
        "\n",
        "In all of the previous notebooks we have performed sentiment analysis on a dataset with only two classes, positive or negative. When we have only two classes our output can be a single scalar, bound between 0 and 1, that indicates what class an example belongs to. When we have more than 2 examples, our output must be a $C$ dimensional vector, where $C$ is the number of classes.\n",
        "\n",
        "In this notebook, we'll be performing classification on a dataset with 6 classes. Note that this dataset isn't actually a sentiment analysis dataset, it's a dataset of questions and the task is to classify what category the question belongs to. However, everything covered in this notebook applies to any dataset with examples that contain an input sequence belonging to one of $C$ classes.\n",
        "\n",
        "Below, we setup the fields, and load the dataset. \n",
        "\n",
        "The first difference is that we do not need to set the `dtype` in the `LABEL` field. When doing a mutli-class problem, PyTorch expects the labels to be numericalized `LongTensor`s. \n",
        "\n",
        "The second different is that we use `TREC` instead of `IMDB` to load the `TREC` dataset. The `fine_grained` argument allows us to use the fine-grained labels (of which there are 50 classes) or not (in which case they'll be 6 classes). You can change this how you please."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvcQuPXnWYXv"
      },
      "source": [
        "Also update to torchtext 0.7.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exWZ_KxtD7XZ",
        "outputId": "556893ca-9514-42a1-8a9f-349f99477804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "! pip install torchtext==0.7.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.7.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.7.0) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.7.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.7.0) (1.6.0+cu101)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.7.0) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.7.0) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXrklVX_D1dh",
        "outputId": "2e1fe086-536a-4637-9f56-b63e026e12aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torchtext\n",
        "torchtext.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.7.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpORoVE4orym",
        "outputId": "6836cbdd-c82c-42ec-9856-f12147b83064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading train_5500.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 1.27MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading TREC_10.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 357kB/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CRnVVEKoryr"
      },
      "source": [
        "Let's look at one of the examples in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3YNxnG0orys",
        "outputId": "dcf311c8-fcc7-419d-d427-cb9a7cec9477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vars(train_data[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'DESC', 'text': ['What', 'is', 'a', 'Cartesian', 'Diver', '?']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwoAQYKforyw"
      },
      "source": [
        "Next, we'll build the vocabulary. As this dataset is small (only ~3800 training examples) it also has a very small vocabulary (~7500 unique tokens), this means we do not need to set a `max_size` on the vocabulary as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3T0BlzUoryw"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2qEeWvvoryz"
      },
      "source": [
        "Next, we can check the labels.\n",
        "\n",
        "The 6 labels (for the non-fine-grained case) correspond to the 6 types of questions in the dataset:\n",
        "- `HUM` for questions about humans\n",
        "- `ENTY` for questions about entities\n",
        "- `DESC` for questions asking you for a description \n",
        "- `NUM` for questions where the answer is numerical\n",
        "- `LOC` for questions where the answer is a location\n",
        "- `ABBR` for questions asking about abbreviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo0t01FhsXMf",
        "outputId": "ea0e2ca5-cc6d-4c52-f16a-4c0924480da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('?', 3743),\n",
              " ('the', 2502),\n",
              " ('What', 2265),\n",
              " ('is', 1165),\n",
              " ('of', 1069),\n",
              " ('in', 791),\n",
              " ('a', 691),\n",
              " ('`', 589),\n",
              " ('How', 512),\n",
              " (\"'s\", 494)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIvkQkOforyz",
        "outputId": "b337d9ac-a9ae-4425-9900-96b415670b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(None, {'HUM': 0, 'ENTY': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXxQA2emory2"
      },
      "source": [
        "As always, we set up the iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XyQLePsory2",
        "outputId": "4745239f-33c6-4b3f-edbc-126001589138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwBwohaIory4"
      },
      "source": [
        "We'll be using the CNN model from the previous notebook, however any of the models covered in these tutorials will work on this dataset. The only difference is now the `output_dim` will be $C$ instead of $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnmKjb4qory5"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        text = text.permute(1, 0)\n",
        "                \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, int(conv.shape[2])).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTw7F_QGory7"
      },
      "source": [
        "We define our model, making sure to set `OUTPUT_DIM` to $C$. We can get $C$ easily by using the size of the `LABEL` vocab, much like we used the length of the `TEXT` vocab to get the size of the vocabulary of the input.\n",
        "\n",
        "The examples in this dataset are generally a lot smaller than those in the IMDb dataset, so we'll use smaller filter sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCLH9-RSory8"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [2,3,4]\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OStrSaZory-"
      },
      "source": [
        "Checking the number of parameters, we can see how the smaller filter sizes means we have about a third of the parameters than we did for the CNN model on the IMDb dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0itli2kTory_",
        "outputId": "0181955b-3ede-4b78-d735-76db00149c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 842,406 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnxVWqREts74",
        "outputId": "10c3520f-44ed-4e98-9ff4-249b72b9f72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "! pip install torchsummaryX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading https://files.pythonhosted.org/packages/36/23/87eeaaf70daa61aa21495ece0969c50c446b8fd42c4b8905af264b40fe7f/torchsummaryX-1.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.6.0+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchsummaryX) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxuz40iHtxBI",
        "outputId": "b5f7d74d-c906-442d-8f30-2cf297dd2a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "source": [
        "from torchsummaryX import summary\n",
        "inputs = torch.zeros((100, 1), dtype=torch.long)\n",
        "summary(model.to(device), inputs.to(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================================================\n",
            "                      Kernel Shape     Output Shape  Params Mult-Adds\n",
            "Layer                                                                \n",
            "0_embedding            [100, 7503]    [1, 100, 100]  750.3k    750.3k\n",
            "1_convs.Conv2d_0  [1, 100, 2, 100]  [1, 100, 99, 1]   20.1k     1.98M\n",
            "2_convs.Conv2d_1  [1, 100, 3, 100]  [1, 100, 98, 1]   30.1k     2.94M\n",
            "3_convs.Conv2d_2  [1, 100, 4, 100]  [1, 100, 97, 1]   40.1k     3.88M\n",
            "4_dropout                        -         [1, 300]       -         -\n",
            "5_fc                      [300, 6]           [1, 6]  1.806k      1.8k\n",
            "------------------------------------------------------------------------\n",
            "                        Totals\n",
            "Total params          842.406k\n",
            "Trainable params      842.406k\n",
            "Non-trainable params       0.0\n",
            "Mult-Adds              9.5521M\n",
            "========================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding</th>\n",
              "      <td>[100, 7503]</td>\n",
              "      <td>[1, 100, 100]</td>\n",
              "      <td>750300.0</td>\n",
              "      <td>750300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_convs.Conv2d_0</th>\n",
              "      <td>[1, 100, 2, 100]</td>\n",
              "      <td>[1, 100, 99, 1]</td>\n",
              "      <td>20100.0</td>\n",
              "      <td>1980000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_convs.Conv2d_1</th>\n",
              "      <td>[1, 100, 3, 100]</td>\n",
              "      <td>[1, 100, 98, 1]</td>\n",
              "      <td>30100.0</td>\n",
              "      <td>2940000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_convs.Conv2d_2</th>\n",
              "      <td>[1, 100, 4, 100]</td>\n",
              "      <td>[1, 100, 97, 1]</td>\n",
              "      <td>40100.0</td>\n",
              "      <td>3880000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 300]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_fc</th>\n",
              "      <td>[300, 6]</td>\n",
              "      <td>[1, 6]</td>\n",
              "      <td>1806.0</td>\n",
              "      <td>1800.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Kernel Shape     Output Shape    Params  Mult-Adds\n",
              "Layer                                                                   \n",
              "0_embedding            [100, 7503]    [1, 100, 100]  750300.0   750300.0\n",
              "1_convs.Conv2d_0  [1, 100, 2, 100]  [1, 100, 99, 1]   20100.0  1980000.0\n",
              "2_convs.Conv2d_1  [1, 100, 3, 100]  [1, 100, 98, 1]   30100.0  2940000.0\n",
              "3_convs.Conv2d_2  [1, 100, 4, 100]  [1, 100, 97, 1]   40100.0  3880000.0\n",
              "4_dropout                        -         [1, 300]       NaN        NaN\n",
              "5_fc                      [300, 6]           [1, 6]    1806.0     1800.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txlUivFBorzB"
      },
      "source": [
        "Next, we'll load our pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWJzr-4ForzC",
        "outputId": "2121ae56-7689-4c32-a45c-a8330413dd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [-0.3110, -0.3398,  1.0308,  ...,  0.5317,  0.2836, -0.0640],\n",
              "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
              "        [ 0.4306,  1.2011,  0.0873,  ...,  0.8817,  0.3722,  0.3458]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjMC0YsSorzE"
      },
      "source": [
        "Then zero the initial weights of the unknown and padding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2lEzcuxorzF"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFtCYpcorzI"
      },
      "source": [
        "Another different to the previous notebooks is our loss function (aka criterion). Before we used `BCEWithLogitsLoss`, however now we use `CrossEntropyLoss`. Without going into too much detail, `CrossEntropyLoss` performs a *softmax* function over our model outputs and the loss is given by the *cross entropy* between that and the label.\n",
        "\n",
        "Generally:\n",
        "- `CrossEntropyLoss` is used when our examples exclusively belong to one of $C$ classes\n",
        "- `BCEWithLogitsLoss` is used when our examples exclusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and $C$ classes (aka multilabel classification)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2mesAsorzJ"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cosjKf0DorzM"
      },
      "source": [
        "Before, we had a function that calculated accuracy in the binary label case, where we said if the value was over 0.5 then we would assume it is positive. In the case where we have more than 2 classes, our model outputs a $C$ dimensional vector, where the value of each element is the beleief that the example belongs to that class. \n",
        "\n",
        "For example, in our labels we have: 'HUM' = 0, 'ENTY' = 1, 'DESC' = 2, 'NUM' = 3, 'LOC' = 4 and 'ABBR' = 5. If the output of our model was something like: **[5.1, 0.3, 0.1, 2.1, 0.2, 0.6]** this means that the model strongly believes the example belongs to class 0, a question about a human, and slightly believes the example belongs to class 3, a numerical question.\n",
        "\n",
        "We calculate the accuracy by performing an `argmax` to get the index of the maximum value in the prediction for each element in the batch, and then counting how many times this equals the actual label. We then average this across the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTOHaJ4AorzN"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3cyGzPuorzP"
      },
      "source": [
        "The training loop is similar to before, without the need to `squeeze` the model predictions as `CrossEntropyLoss` expects the input to be **[batch size, n classes]** and the label to be **[batch size]**.\n",
        "\n",
        "The label needs to be a `LongTensor`, which it is by default as we did not set the `dtype` to a `FloatTensor` as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ihv-pLSuEKm"
      },
      "source": [
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4iKH0WKuKE-",
        "outputId": "bbd4106c-1b6a-4b0f-86d6-b1144f3ea90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "batch.label, batch.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2, 2, 4, 2, 2, 1, 2, 0, 1, 3, 1, 1, 1, 3, 0, 3, 0, 2, 2, 4, 1, 1, 4, 0,\n",
              "         0, 0, 4, 1, 0, 0, 3, 1, 1, 2, 3, 4, 3, 1, 1, 1, 2, 0, 2, 0, 1, 1, 1, 3,\n",
              "         0, 1, 0, 1, 2, 0, 0, 2, 0, 1, 3, 0, 1, 1, 1, 1], device='cuda:0'),\n",
              " tensor([[   4,   10,   52,  ...,    4,   51,    4],\n",
              "         [   5,   22,    3,  ..., 6069,  593,  276],\n",
              "         [ 997,   28,   66,  ...,  530,   30,   21],\n",
              "         ...,\n",
              "         [   1,    1,    1,  ...,    1,    1,    1],\n",
              "         [   1,    1,    1,  ...,    1,    1,    1],\n",
              "         [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imc8CHkHuUb6",
        "outputId": "7be87de3-c8b7-4e8a-c2b1-5fe880e15510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "batch.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   4,   10,   52,  ...,    4,   51,    4],\n",
              "        [   5,   22,    3,  ..., 6069,  593,  276],\n",
              "        [ 997,   28,   66,  ...,  530,   30,   21],\n",
              "        ...,\n",
              "        [   1,    1,    1,  ...,    1,    1,    1],\n",
              "        [   1,    1,    1,  ...,    1,    1,    1],\n",
              "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_2OCQ0ForzP"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "        batch.text = batch.text.to(device)\n",
        "        batch.label = batch.label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOMYrFsBorzS"
      },
      "source": [
        "The evaluation loop is, again, similar to before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eokznx-lorzT"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jtAsA-orzV"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJxrbPtEorzY"
      },
      "source": [
        "Next, we train our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KNAi5T5qorzY",
        "outputId": "b5fcafa4-6dbf-4a2b-e822-c82243f798c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        }
      },
      "source": [
        "N_EPOCHS = 15\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.032 | Train Acc: 99.41%\n",
            "\t Val. Loss: 0.445 |  Val. Acc: 86.06%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.027 | Train Acc: 99.53%\n",
            "\t Val. Loss: 0.446 |  Val. Acc: 86.28%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.023 | Train Acc: 99.58%\n",
            "\t Val. Loss: 0.452 |  Val. Acc: 86.05%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.026 | Train Acc: 99.43%\n",
            "\t Val. Loss: 0.457 |  Val. Acc: 85.81%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.016 | Train Acc: 99.79%\n",
            "\t Val. Loss: 0.458 |  Val. Acc: 85.80%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.015 | Train Acc: 99.77%\n",
            "\t Val. Loss: 0.463 |  Val. Acc: 86.22%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.014 | Train Acc: 99.87%\n",
            "\t Val. Loss: 0.467 |  Val. Acc: 86.64%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.011 | Train Acc: 99.95%\n",
            "\t Val. Loss: 0.472 |  Val. Acc: 86.41%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.71%\n",
            "\t Val. Loss: 0.478 |  Val. Acc: 86.38%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.012 | Train Acc: 99.77%\n",
            "\t Val. Loss: 0.481 |  Val. Acc: 86.14%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.013 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.490 |  Val. Acc: 86.05%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.90%\n",
            "\t Val. Loss: 0.504 |  Val. Acc: 86.10%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.008 | Train Acc: 99.84%\n",
            "\t Val. Loss: 0.483 |  Val. Acc: 86.30%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.489 |  Val. Acc: 86.06%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.007 | Train Acc: 99.92%\n",
            "\t Val. Loss: 0.499 |  Val. Acc: 86.36%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgX-Xw4Vorzb"
      },
      "source": [
        "Finally, let's run our model on the test set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVQwjGUMorzb",
        "outputId": "1a586eee-a1cf-4355-e829-2ec58aff4c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.275 | Test Acc: 89.05%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdcIKVxkorzd"
      },
      "source": [
        "Similar to how we made a function to predict sentiment for any given sentences, we can now make a function that will predict the class of question given.\n",
        "\n",
        "The only difference here is that instead of using a sigmoid function to squash the input between 0 and 1, we use the `argmax` to get the highest predicted class index. We then use this index with the label vocab to get the human readable label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_JXA4eQorze"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_class(model, sentence, min_len = 4):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    preds = model(tensor)\n",
        "    max_preds = preds.argmax(dim = 1)\n",
        "    return max_preds.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWQ3HSliA_Df",
        "outputId": "a8587a45-a9ee-4d77-c970-8d82ab54cdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(nlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SbepQ6xO4G"
      },
      "source": [
        "sentence = 'how old are you?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v45EZ7zxRqU",
        "outputId": "18b20e22-fc52-4f25-8eac-3338e9edaabc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "tokenized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'old', 'are', 'you', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5_4715HxZCp",
        "outputId": "fcbeb44d-232e-4825-ab91-524672834383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "indexed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[159, 172, 18, 31, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlWdIILtxizb",
        "outputId": "c0bdec6c-ca20-4a88-bb0a-5513c4abdc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tensor = torch.LongTensor(indexed).to(device)\n",
        "tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([159, 172,  18,  31,   2], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eWAosZJXjL",
        "outputId": "185c2529-3344-4acb-a248-da4a3286e31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "predicted = model(tensor.unsqueeze(1).to('cpu')).squeeze(0)\n",
        "predicted = F.softmax(predicted)\n",
        "predicted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0390, 0.1320, 0.2520, 0.4940, 0.0639, 0.0191],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNxcYQvWxliQ",
        "outputId": "831bd8cf-321c-406e-83a3-483dd47fa730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sorted_values = predicted.argsort(descending=True).cpu().numpy()\n",
        "sorted_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 1, 4, 0, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sl2amBdI6QK",
        "outputId": "6ad5ed80-cbc0-4cb5-9cd2-be40256a1e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "list(map(lambda x: { \"label_idx\": x.item(), \"label_name\": LABEL.vocab.itos[x], 'confidence': predicted[x].item() } , sorted_values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'confidence': 0.4940268099308014, 'label_idx': 3, 'label_name': 'NUM'},\n",
              " {'confidence': 0.25197815895080566, 'label_idx': 2, 'label_name': 'DESC'},\n",
              " {'confidence': 0.13199791312217712, 'label_idx': 1, 'label_name': 'ENTY'},\n",
              " {'confidence': 0.06391700357198715, 'label_idx': 4, 'label_name': 'LOC'},\n",
              " {'confidence': 0.03897593542933464, 'label_idx': 0, 'label_name': 'HUM'},\n",
              " {'confidence': 0.019104206934571266, 'label_idx': 5, 'label_name': 'ABBR'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tFXYsIcorzh"
      },
      "source": [
        "Now, let's try it out on a few different questions..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBeQzjm8orzh",
        "outputId": "febef046-74d7-48e1-bbac-80c5a6f83513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred_class = predict_class(model, \"Who is Keyser Söze?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 0 = HUM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coOB8mHkorzj",
        "outputId": "0b02277e-1561-4a46-bb03-c8cd618f3d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred_class = predict_class(model, \"How many minutes are in six hundred and eighteen hours?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 3 = NUM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of_WWebXorzn",
        "outputId": "436fb5c6-8c43-4f1f-e8bf-9fd48f8c3da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred_class = predict_class(model, \"What continent is Bulgaria in?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 4 = LOC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkpRKQ91orzq",
        "outputId": "86eeec08-30ed-4e69-c48e-f550d5ecf5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred_class = predict_class(model, \"What does WYSIWYG stand for?\")\n",
        "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class is: 5 = ABBR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaEnFZF_ssjA"
      },
      "source": [
        "## Save the Model and the Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lPRe9ZssDDz"
      },
      "source": [
        "def save_vocab(vocab, path):\n",
        "    import pickle\n",
        "    output = open(path, 'wb')\n",
        "    pickle.dump(vocab, output)\n",
        "    output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FxTxn1kr1jM"
      },
      "source": [
        "torch.save(model, 'conv-sentimental-mclass.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ats8t6Sfs6t8"
      },
      "source": [
        "save_vocab({ 'TEXT.vocab': TEXT.vocab, 'LABEL.vocab': LABEL.vocab }, 'conv-sentimental-vocab.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iSdihj9WJ6G"
      },
      "source": [
        "We need to use scripted model since traced model will make the shapes constant and we wont be able to use variable length strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCGfqINlqI51"
      },
      "source": [
        "scripted_model = torch.jit.script(model.to('cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbDVwU3MrXuQ",
        "outputId": "57b02f60-a92b-42d8-a9f9-bad9de128592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "scripted_model(torch.zeros((5, 1), dtype=torch.long))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0919,  0.0037,  0.0381, -0.0365,  0.0261, -0.0287]],\n",
              "       grad_fn=<DifferentiableGraphBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8lpeTgnrY8X"
      },
      "source": [
        "scripted_model.save('conv-sentimental-mclass.scripted.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgjylhM2rlPS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}